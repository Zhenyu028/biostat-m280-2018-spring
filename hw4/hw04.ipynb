{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 4\n",
    "\n",
    "**Due June 12 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we build a classifer for handwritten digit recognition. Following figure shows example bitmaps of handwritten digits from U.S. postal envelopes. \n",
    "\n",
    "<img src=\"./handwritten_digits.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "Each digit is represented by a $32 \\times 32$ bitmap in which each element indicates one pixel with a value of white or black. Each $32 \\times 32$ bitmap is divided into blocks of $4 \\times 4$, and the number of white pixels are counted in each block. Therefore each handwritten digit is summarized by a vector $\\mathbf{x} = (x_1, \\ldots, x_{64})$ of length 64 where each element is a count between 0 and 16. \n",
    "\n",
    "We will use a model-based method by assuming a distribution on the count vector and carry out classification using probabilities. A common distribution for count vectors is the multinomial distribution. However as you will see in Q10, it is not a good model for handwritten digits. Let's work on a more flexible model for count vectors. In the Dirichlet-multinomial model, we assume the multinomial probabilities $\\mathbf{p} = (p_1,\\ldots, p_d)$ follow a Dirichlet distribution with parameter vector $\\alpha = (\\alpha_1,\\ldots, \\alpha_d)$, $\\alpha_j>0$, and density\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "For a multivariate count vector $\\mathbf{x}=(x_1,\\ldots,x_d)$ with batch size $|\\mathbf{x}|=\\sum_{j=1}^d x_j$, show that the probability mass function for Dirichlet-multinomial distribution is\n",
    "$$\n",
    "    f(\\mathbf{x} \\mid \\alpha) \n",
    "\t= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p}  \n",
    "    = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$\n",
    "where $\\Delta_d$ is the unit simplex in $d$ dimensions and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question1. To get the distribution of x given $\\alpha$, we can include the parameter p first then integrate it out**\n",
    "$$\n",
    "\\begin{align}\n",
    "  f(\\mathbf{x} \\mid \\alpha)  &= Pr(\\mathbf{x},\\mathbf{p} \\mid \\alpha) d \\mathbf{p} \\\\\n",
    " & = Pr(\\mathbf{p} \\mid \\alpha) Pr(\\mathbf{x} \\mid \\mathbf{p},\\alpha)d \\mathbf{p} \n",
    "\\end{align}\n",
    "$$\n",
    "where $Pr(\\mathbf{p} \\mid \\alpha) = \\pi(\\mathbf{p})$ and $Pr(\\mathbf{x} \\mid \\mathbf{p},\\alpha) = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} $\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x} \\mid \\alpha) \n",
    "\t& = \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p}\\\\\n",
    "    & = \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j}  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} d \\mathbf{p}\\\\\n",
    "    & = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j} \\prod_{j=1}^d p_j^{\\alpha_j-1} d \\mathbf{p}\\\\\n",
    "    & = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j+ \\alpha_j-1} d \\mathbf{p}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last integral contains the kernel of a Dirichlet distribution, thus can be integrated out and only constant term left,\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x} \\mid \\alpha) = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Given independent data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, show that the log-likelihood is\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "Is the log-likelihood a concave function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 2.** For one data point $\\mathbf{x}$, from $L(\\alpha)  = log(f(\\mathbf{x} \\mid \\alpha))$ it is trivial that the log-likelihood is\n",
    "$$\n",
    "L(\\alpha) =  \\ln \\binom{|\\mathbf{x}|}{\\mathbf{x}} +  \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] -  [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "Then for $n$ independent data points, their joint log-likelihood is sum of $n$ identical log-denisities,\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "Is it a concave function? Remained to be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Write Julia function to compute the log-density of the Dirichlet-multinomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "    logpdf = 0.0\n",
    "    \n",
    "    t = 0.0\n",
    "    for i in 1:length(x)\n",
    "        t += lgamma(α[i] + x[i]) - lgamma(α[i]) - lfact(x[i])\n",
    "    end\n",
    "    \n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    logpdf = t + lfact(sumx) + lgamma(sumα) - lgamma(sumx + sumα)\n",
    "    \n",
    "    return logpdf\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "    return sum(r)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Each row contains the 64 counts of a digit and the last element (65th element) indicates what digit it is. For grading purpose, evaluate the total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ using your function in Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.073802 seconds (30.57 k allocations: 3.473 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-638817.993292528"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata = readcsv(\"optdigits.tra\", Int)\n",
    "digits = traindata[:, end]\n",
    "counts = traindata[:, 1:64]'\n",
    "α = ones(size(counts,1))\n",
    "\n",
    "# the total log-likelihood\n",
    "@time dirmult_logpdf(counts, α)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Derive the score function $\\nabla L(\\alpha)$, observed information matrix $-d^2L(\\alpha)$, and Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ for the Dirichlet-multinomial distribution.\n",
    "\n",
    "Comment on why Fisher scoring method is inefficient for computing MLE in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 5 - $\\nabla L(\\alpha)$** \n",
    "\n",
    "Given \n",
    "\n",
    "$$\n",
    "L(\\alpha) =  \\ln \\binom{|\\mathbf{x}|}{\\mathbf{x}} +  \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] -  [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "\n",
    "The score function $\\nabla L(\\alpha)$ is a vector with entry\n",
    "\n",
    "$$\n",
    "\\nabla L(\\alpha) = \\frac{\\partial}{\\partial \\alpha_j}L(\\alpha)= \\sum_{i=1}^n  [ \\psi (\\alpha_j + x_{ij}) - \\psi (\\alpha_j)] - \\sum_{i=1}^n [\\psi(|\\alpha|+|\\mathbf{x}_i|) - \\psi(|\\alpha|)].\n",
    "$$\n",
    "\n",
    "$\\psi(x)=\\frac{d}{dx}\\ln\\big(\\Gamma(x)\\big)=\\frac{\\Gamma'(x)}{\\Gamma(x)}$ is the digamma function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 5- observed information matrix $ -d^2L(\\alpha)$** \n",
    "\n",
    "Observed information matrix $-d^2L(\\alpha)$ is negative Hessian matrix, \n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_1^2} & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_n} \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_n} \\\\[2.2ex]\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To take the second derivative of log-gamma function we need to deal with polygamma function: https://en.wikipedia.org/wiki/Polygamma_function\n",
    "\n",
    "$$\n",
    "\\psi^{(m)}(x) := \\frac{d^m}{dx^m} \\psi(x) = \\frac{d^{m+1}}{dx^{m+1}} \n",
    "$$\n",
    "\n",
    "With the gradient calculated already, the diagonal entries of Hessian matrix contain polygamma function from both summations, and the off-diagonal entries contain polygamma function from only the second summation. $\\psi^{(1)}$ is so-called trigamma function.\n",
    "\n",
    "$$H_{jj} = \\dfrac{\\partial^2 L(\\alpha)}{\\partial \\alpha_j^2} =\\sum_{i=1}^n  [ \\psi^{(1)} (\\alpha_j + x_{ij}) - \\psi^{(1)} (\\alpha_j)] - \\sum_{i=1}^n [\\psi^{(1)}(|\\alpha|+|\\mathbf{x}_i|) - \\psi^{(1)}(|\\alpha|)] $$\n",
    "$$ H_{ji} = \\dfrac{\\partial^2 L(\\alpha)}{\\partial \\alpha_j \\partial \\alpha_i} = - \\sum_{i=1}^n [\\psi^{(1)}(|\\alpha|+|\\mathbf{x}_i|) - \\psi^{(1)}(|\\alpha|)] ,j \\neq i, \\text{the off-diagonals are constant}$$\n",
    "\n",
    "So the Hessian matrix is in a format that we can rewrite it by a diagnoal matrix subtracting a rank-one matrix with all entries equal to $\\sum_{i=1}^n [\\psi^{(1)}(|\\alpha|+|\\mathbf{x}_i|) - \\psi^{(1)}(|\\alpha|)]$.\n",
    "\n",
    "Polygamma function has a property that enables us to simplify the summation $\\sum_{i=1}^n  [ \\psi^{(1)} (\\alpha_j + x_{ij}) - \\psi^{(1)} (\\alpha_j)] $ and $\\sum_{i=1}^n [\\psi^{(1)}(|\\alpha|+|\\mathbf{x}_i|) - \\psi^{(1)}(|\\alpha|)]$: its recurrence relation as below\n",
    "\n",
    "$$\n",
    "\\psi^{(m)}(x+1)= \\psi^{(m)}(x) + \\frac{(-1)^m\\,m!}{x^{m+1}}\n",
    "$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\n",
    "\\psi^{(1)} (\\alpha_j + x_{ij}) - \\psi^{(1)} (\\alpha_j) =  \\sum_{k = 1}^{k = x_{ij} - 1} - \\frac{1}{ (\\alpha_j + k)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\psi^{(1)}(|\\alpha|+|\\mathbf{x}_i|) - \\psi^{(1)}(|\\alpha|) = \\sum_{k = 1}^{k = |\\mathbf{x}_i| - 1} - \\frac{1}{ (\\alpha_j + k)^2}\n",
    "$$\n",
    "\n",
    "In the end,\n",
    "$$\n",
    "-d^2L(\\alpha) = - H = \\mathbf{D} - c \\mathbf{1}\\mathbf{1}^T\n",
    "$$\n",
    "where $\\mathbf{1}$ is all-one vector. Diagonal matrix $\\mathbf{D}$ has entries $\\mathbf{D}_{j} = \\sum_{i=1}^n \\sum_{k = 1}^{k = x_{ij} - 1}  \\frac{1}{ (\\alpha_j + k)^2}$, and $c =  \\sum_{i=1}^n \\sum_{k = 1}^{k = |\\mathbf{x}_i| - 1}  \\frac{1}{ (|\\alpha| + k)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** Fisher information matrix $FI = \\mathbf{E}[-d^2L(\\alpha)]$\n",
    "$$\n",
    "FI = \\mathbf{E}[\\mathbf{D}] - c \\mathbf{1}\\mathbf{1}^T\n",
    "$$\n",
    "\n",
    "$\\mathbf{E}[\\mathbf{D}]$ is not easy to evaluate. So we turn to Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "What structure does the observed information matrix possess that can facilitate the evaluation of the Newton direction? Is the observed information matrix always positive definite? What remedy can we take if it fails to be positive definite? (Hint: HW1 Q6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 6** The \"Diagonal matrix - rank 1 matrix\" structure of observed information matrix makes it easier to get its inverse matrix in the Newton's method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix is P.D. if and only if all of its upper left determinants are positive. By **matrix determinant lemma**,\n",
    "\n",
    "$$\n",
    "det(-d^2L(\\alpha)) = det(\\mathbf{D})det(1 - c \\sum_j \\mathbf{D}_j^{-1}) = (\\Pi_j \\mathbf{D}_j )(1 - c \\sum_j \\mathbf{D}_j^{-1})\n",
    "$$\n",
    "Since $\\mathbf{D}_j > 0$ for $j = 1 \\dots d$, the matrix is positive definite if and only if $c < [\\sum_j \\mathbf{D}_j^{-1}]^{-1}$. It is not easy see whether this inequality always holds. To be save, if it is not true, we can shrink make $c = 0.99 \\times [\\sum_j \\mathbf{D}_j^{-1}]^{-1}$. In the Newton's method implemented in later question, if the inequality ever fails to hold, a notice will be printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Discuss how to choose a good starting point. Implement this as the default starting value in your function below. (Hint: Method of moment estimator may furnish a good starting point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 7** The method of moment estimator involves equating sample moments with theoretical moments and solve the equation for parameter.\n",
    "\n",
    "$\\mathbf{E}[P_j] = \\frac{\\alpha_j}{|\\alpha|}$, sample 1th moment $= \\frac{1}{n}\\sum_{i = 1}^n \\frac{x_{ij}}{|\\mathbf{x}_i|}$, $\\mathbf{E}[P_j^2] = \\frac{\\alpha_j(\\alpha_j + 1)}{|\\alpha|(|\\alpha| + 1)}$, sample 2nd moment $ = \\frac{1}{n}\\sum_{i = 1}^n (\\frac{x_{ij}}{|\\mathbf{x}_i|})^2$\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbf{E}[P_j^2]}{\\mathbf{E}[P_j]} = \\frac{\\alpha_j + 1}{|\\mathbf{\\alpha} + 1|} = \\frac{\\sum_{i = 1}^n (\\frac{x_{ij}}{|\\mathbf{x}_i|})^2}{\\sum_{i = 1}^n \\frac{x_{ij}}{|\\mathbf{x}_i|}}\n",
    "$$\n",
    "\n",
    "Thus \n",
    "$$\n",
    "\\sum_{j = 1}^d \\frac{\\mathbf{E}[P_j^2]}{\\mathbf{E}[P_j]} = \\sum_{j = 1}^d \\frac{\\mathbf{E}[P_j^2]}{\\mathbf{E}[P_j]} = \\frac{|\\mathbf{\\alpha}| + d}{|\\mathbf{\\alpha}| + 1} = \\sum_{j = 1}^d \\frac{\\sum_{i = 1}^n (\\frac{x_{ij}}{|\\mathbf{x}_i|})^2}{\\sum_{i = 1}^n \\frac{x_{ij}}{|\\mathbf{x}_i|}}\n",
    "$$\n",
    "Now we can solve for $|\\mathbf{\\alpha}|,  |\\mathbf{\\alpha}| = \\frac{d - k}{k - 1} = |\\mathbf{\\alpha}| ^{(0)}$, where $k = \\sum_{j = 1}^d \\frac{\\sum_{i = 1}^n (\\frac{x_{ij}}{|\\mathbf{x}_i|})^2}{\\sum_{i = 1}^n \\frac{x_{ij}}{|\\mathbf{x}_i|}}$\n",
    "\n",
    "From $\\alpha_j = \\mathbf{E}[P_j] \\times |\\alpha|$ we can take $\\frac{\\sum_{i = 1}^n x_{ij}}{\\sum_{i=1}^n |\\mathbf{x}_i|}$ as a weight for $\\alpha_j$, so to initialize \n",
    "$$\n",
    " \\alpha_j^{(0)} =  |\\alpha|^{(0)}\\frac{\\sum_{i = 1}^n x_{ij}}{\\sum_{i=1}^n |\\mathbf{x}_i|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By **Sherman-Morrison formula** \n",
    "$$\n",
    "\\begin{align}\n",
    "[-d^2L(\\alpha)]^{-1} & =  [\\mathbf{D} - c \\mathbf{1}\\mathbf{1}^T]^{-1} \\\\\n",
    "& = \\mathbf{D}^{-1} + \\frac{1}{c^{-1} - \\sum_j \\mathbf{D}_j^{-1}} \\mathbf{D}^{-1} \\mathbf{1}\\mathbf{1}^T \\mathbf{D}^{-1} \n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(dirmult_startingvalue_mom(counts)) = Array{Float64,1}\n",
      "(typeof(t0), typeof(α)) = (Array{Float64,1}, Array{Float64,1})\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: dirmult_score not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: dirmult_score not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "\n",
    "function dirmult_startingvalue_mom(X::Matrix)\n",
    "    \n",
    "    d, n = size(X)\n",
    "    αsum_ini = ones(64)\n",
    "    \n",
    "    vec = zeros(n)\n",
    "    k = 0.0\n",
    "    \n",
    "    for j in 1:d\n",
    "        \n",
    "        vec = X[j,:]'./ sum(X, 1)\n",
    "        num = sum(vec.^2)\n",
    "        denum = sum(vec)\n",
    "    \n",
    "        if denum > 0\n",
    "            k += num / denum\n",
    "        end\n",
    "    end\n",
    "\n",
    "    αsum_ini = (d - k) / (k - 1.0)\n",
    "\n",
    "    grandsum = sum(X)\n",
    "    \n",
    "    α_ini = zeros(Float64, d)\n",
    "    \n",
    "    for i in 1:d\n",
    "        α_ini[i] = αsum_ini * sum(X, 2)[i] / grandsum\n",
    "    end\n",
    "\n",
    "    return α_ini\n",
    "end\n",
    "\n",
    "@show typeof(dirmult_startingvalue_mom(counts))\n",
    "t0 = dirmult_startingvalue_mom(counts)\n",
    "α = ones(size(counts,1))\n",
    "@show typeof(t0), typeof(α)\n",
    "\n",
    "dirmult_score(counts, t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using the Newton's method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 8** First need to write functions to evaluate score function and observed information matrix for Dirichlet-multinomial distribution, and second, get the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "using SpecialFunctions\n",
    "\n",
    "function dirmult_score(x::Vector, α::Vector)\n",
    "    \n",
    "    score = zeros(length(α))\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    for j in 1:length(x)\n",
    "        #println(\"length(x) is\", length(x))\n",
    "        score[j] = digamma(α[j] + x[j]) - digamma(α[j]) \n",
    "    end\n",
    "    \n",
    "    common_term = digamma(sumx + sumα) - digamma(sumα)\n",
    "\n",
    "    score .= score - common_term\n",
    "    return score\n",
    "end\n",
    "\n",
    "function dirmult_score!(s::Vector, X::Matrix, α::Vector)\n",
    "    d, n = size(X)\n",
    "    for i in 1:d\n",
    "        for j in 1:n\n",
    "            s[i] += dirmult_score(X[:, j], α)[i]\n",
    "        end \n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "\n",
    "function dirmult_score(X::Matrix, α::Vector)\n",
    "    s = zeros(size(X, 1))\n",
    "    dirmult_score!(s, X, α)\n",
    "    return s\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_obsinfo (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SpecialFunctions\n",
    "\n",
    "function dirmult_obsinfo(x::Vector, α::Vector)\n",
    "    \n",
    "    diag_matrix = zeros(length(α))\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    for i in 1:length(x)\n",
    "        diag_matrix[i] = trigamma(α[i]) - trigamma(α[i] + x[i]) \n",
    "    end\n",
    "    \n",
    "    constant_term = trigamma(sumα) - trigamma(sumx + sumα) \n",
    "    return diag_matrix, constant_term\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(obsinfo::Vector, X::Matrix, α::Vector)\n",
    "    \n",
    "    constant = 0.0\n",
    "    for j in 1:size(X, 2)\n",
    "        for i in 1:size(X, 1)\n",
    "            obsinfo[i] += dirmult_obsinfo(X[:, j], α)[1][i]\n",
    "        end \n",
    "        constant += dirmult_obsinfo(X[:, j], α)[2]\n",
    "    end\n",
    "    return obsinfo, constant\n",
    "end\n",
    "\n",
    "\n",
    "function dirmult_obsinfo(X::Matrix, α::Vector)\n",
    "    obsinfo = zeros(size(X, 1))\n",
    "    dirmult_obsinfo!(obsinfo, X, α)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(constantC, tmp) = (42.98980958419128, 47.14466377349657)\n",
      "(constantC, tmp) = (42.98980958419128, 47.14466377349657)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62-element Array{Float64,1}:\n",
       " 1.85573e10\n",
       " 4.4733e8  \n",
       " 2.03105e8 \n",
       " 2.08591e8 \n",
       " 3.97094e8 \n",
       " 1.44277e9 \n",
       " 1.59756e10\n",
       " 1.94195e12\n",
       " 1.36691e9 \n",
       " 2.17318e8 \n",
       " 2.09282e8 \n",
       " 2.2682e8  \n",
       " 2.63606e8 \n",
       " ⋮         \n",
       " 2.43337e8 \n",
       " 2.34357e8 \n",
       " 5.47553e8 \n",
       " 2.63129e10\n",
       " 4.14282e13\n",
       " 1.69955e10\n",
       " 4.03642e8 \n",
       " 1.99353e8 \n",
       " 2.02082e8 \n",
       " 3.20312e8 \n",
       " 9.47898e8 \n",
       " 1.43774e10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    rowind = find(sum(counts, 2))\n",
    "    colind = find(sum(counts, 1))\n",
    "    Xwork = counts[rowind, colind] \n",
    "  \n",
    "    α = dirmult_startingvalue_mom(Xwork)\n",
    "    d, n = size(Xwork)\n",
    "    v = zeros(d)\n",
    "#@time obsinfodiag, c = dirmult_obsinfo!(v, counts, α)\n",
    "    α = dirmult_startingvalue_mom(Xwork)\n",
    "\n",
    "    diagoals, constantC = dirmult_obsinfo!(v, Xwork, α)\n",
    "    \n",
    "tmp = 1.0 / sum(diagoals.^(-1))\n",
    "\n",
    "@show constantC, tmp\n",
    "        if constantC >= tmp\n",
    "            constantC = 0.99 * tmp\n",
    "        end\n",
    "@show constantC, tmp\n",
    "        t = 1.0 ./ diagoals\n",
    "Diagonal(t)\n",
    "diagoals * diagoals'/2 * α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: Xd not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: Xd not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "   X = Xd \n",
    "rowind = find(sum(X, 2))\n",
    "    colind = find(sum(X, 1))\n",
    "    X = X[rowind, colind] \n",
    "     \n",
    "    d, n = size(X)\n",
    "    @show d, n\n",
    "    \n",
    "    # set default starting point from Q7\n",
    "    α = dirmult_startingvalue_mom(X)\n",
    "    @show size(α)\n",
    "    αnew = similar(α)\n",
    "    \n",
    "    # preallocation: score, obsinfo diagonals & constant, \n",
    "    ∇ = zeros(d)\n",
    "    diagonals = zeros(d)\n",
    "    invdiagonals = zeros(d)\n",
    "    constantC = 0.0\n",
    "    invconstantC = 0.0\n",
    "    @show loglike_old = dirmult_logpdf(X, α)\n",
    "\n",
    "    ###############evaluate gradient (score)\n",
    "    dirmult_score!(∇, X, α) #64 element array \n",
    "        \n",
    "    # compute Newton's direction\n",
    "    ############### first get the P.D. approximation for obs info\n",
    "    ############### obs info\n",
    "        diagoals, constantC = dirmult_obsinfo!(diagonals, X, α)\n",
    "        invdiagonals = 1.0 ./ diagoals\n",
    "        invconstantC = 1.0 / constantC\n",
    "        \n",
    "        tmp = sum(invdiagonals)\n",
    "        if invconstantC <= tmp\n",
    "            invconstantC = 1.01 * tmp\n",
    "        end\n",
    "        \n",
    "        # now A = D - c 11', the newton direction = A^(-1) * score, solve linear system \n",
    "        @show size(invdiagonals)\n",
    "        @show size(Diagonal(invdiagonals))\n",
    "        @show size(invdiagonals * invdiagonals')\n",
    "        @show invconstantC - tmp\n",
    "        @show size(∇)\n",
    "\n",
    "        \n",
    "\n",
    "newton_direction = (Diagonal(invdiagonals) + (invdiagonals * invdiagonals') ./ (invconstantC - tmp) )* ∇\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, loglikelihood new = -37469.06702100788\n",
      "iteration 2, loglikelihood new = -37462.83923113064\n",
      "iteration 3, loglikelihood new = -37462.325913881905\n",
      "iteration 4, loglikelihood new = -37461.258273711304\n",
      "iteration 5, loglikelihood new = -37457.29898974438\n",
      "iteration 6, loglikelihood new = -37447.5480391979\n",
      "iteration 7, loglikelihood new = -37434.93948110486\n",
      "iteration 8, loglikelihood new = -37422.944592434855\n",
      "iteration 9, loglikelihood new = -37413.67300104533\n",
      "iteration 10, loglikelihood new = -37407.74896318738\n",
      "iteration 11, loglikelihood new = -37404.76335430404\n",
      "iteration 12, loglikelihood new = -37403.779554762674\n",
      "iteration 13, loglikelihood new = -37403.72661260617\n",
      "iteration 14, loglikelihood new = -37403.65433733233\n",
      "iteration 15, loglikelihood new = -37402.87065998386\n",
      "iteration 16, loglikelihood new = -37400.99113454593\n",
      "iteration 17, loglikelihood new = -37397.92813664635\n",
      "iteration 18, loglikelihood new = -37393.8419691847\n",
      "iteration 19, loglikelihood new = -37389.07070752078\n",
      "iteration 20, loglikelihood new = -37384.05125902144\n",
      "iteration 21, loglikelihood new = -37379.24107451736\n",
      "iteration 22, loglikelihood new = -37375.04801811876\n",
      "iteration 23, loglikelihood new = -37371.77458302041\n",
      "iteration 24, loglikelihood new = -37369.58153892799\n",
      "iteration 25, loglikelihood new = -37368.47498175023\n",
      "iteration 26, loglikelihood new = -37368.31907112835\n",
      "iteration 27, loglikelihood new = -37368.31926544397\n",
      "  2.795456 seconds (4.26 M allocations: 1.338 GiB, 2.41% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-37368.31926544397, 26, [0.0, 0.0318719, 4.58746, 13.6705, 11.1672, 2.24995, 0.0584452, 0.0, 0.0, 0.939267  …  2.37487, 0.0, 0.0, 0.0189598, 4.61741, 14.1105, 13.8349, 5.34713, 0.181637, 0.0], [0.0, 26.241, 0.270343, 0.304147, 0.105527, 0.201684, 4.72773, 0.0, 0.0, 0.706559  …  -0.0349898, 0.0, 0.0, 7.1146, 0.343714, 0.364767, 0.324929, -0.208318, -0.05869, 0.0], [0.0 0.0 … 0.0 0.0; 0.0 270247.0 … -0.574381 0.0; … ; 0.0 -0.574381 … 45182.4 0.0; 0.0 0.0 … 0.0 0.0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "function dirmult_newton(X::Matrix;\n",
    "            printout = false,\n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-6,\n",
    "            α0::Vector{Float64} = dirmult_startingvalue_mom(X))\n",
    "    \n",
    "    rowindex = find(sum(X, 2))\n",
    "    colind = find(sum(X, 1))\n",
    "    X = X[rowindex, colind] \n",
    "     \n",
    "    d, n = size(X)\n",
    "\n",
    "    \n",
    "    # set default starting point from Q7\n",
    "    α = α0[rowindex]\n",
    "    αnew = similar(α)\n",
    "    \n",
    "    # preallocation: score, obsinfo diagonals & constant, \n",
    "    ∇ = zeros(d)\n",
    "    diagonals = zeros(d)\n",
    "    invdiagonals = zeros(d)\n",
    "    \n",
    "    constantC = 0.0\n",
    "    invconstantC = 0.0\n",
    "    loglike_new = 0.0\n",
    "    \n",
    "\n",
    "    loglike_loop = dirmult_logpdf(X, α)\n",
    "    \n",
    "    # Newton loop\n",
    "    num_iter = 0\n",
    "    for iter in 1:maxiters\n",
    "        \n",
    "        ###############evaluate gradient (score)\n",
    "        dirmult_score!(∇, X, α) #64 element array \n",
    "        \n",
    "        # compute Newton's direction\n",
    "        ############### first get the P.D. approximation for obs info\n",
    "        ############### obs info\n",
    "        diagoals, constantC = dirmult_obsinfo!(diagonals, X, α)\n",
    "        invdiagonals = 1.0 ./ diagoals\n",
    "        invconstantC = 1.0 / constantC\n",
    "        \n",
    "        tmp = sum(invdiagonals)\n",
    "        if invconstantC <= tmp\n",
    "            invconstantC = 1.05 * tmp\n",
    "            print(\"dsafs\")\n",
    "        end\n",
    "        \n",
    "        # now A = D - c 11', the newton direction = A^(-1) * score, solve linear system \n",
    "        \n",
    "        newton_direction = (Diagonal(invdiagonals) + invdiagonals * invdiagonals' / (invconstantC - tmp))* ∇\n",
    "\n",
    "\n",
    "        # line search loop\n",
    "        step = 1.0\n",
    "        \n",
    "        for i in eachindex(α)       \n",
    "            if newton_direction[i] < 0\n",
    "                step = min(- α[i] / newton_direction[i] * 0.95, step)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for lsiter in 1:10\n",
    "            \n",
    "            αnew .= α .+ step .* newton_direction\n",
    "            loglike_new = dirmult_logpdf(X, αnew)\n",
    "         \n",
    "            if loglike_new > loglike_loop\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            if lsiter < 10\n",
    "                step /= 2.0\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        copy!(α, αnew)\n",
    "        \n",
    "        loglike_old = loglike_loop\n",
    "        loglike_loop = loglike_new\n",
    "\n",
    "        if printout\n",
    "            println(\"iteration \", iter, \", loglikelihood new = \", loglike_new)\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(loglike_new - loglike_old) < tolfun * (abs(loglike_old) + 1)\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        num_iter += 1  \n",
    "    end\n",
    "    \n",
    "        lastα = zeros(length(α0))\n",
    "        lastα[rowindex] = α\n",
    "    \n",
    "        last∇ = zeros(length(α0))\n",
    "        last∇[rowindex] = dirmult_score(X, α)\n",
    "    \n",
    "        obsinfo = zeros(length(α0), length(α0))\n",
    "        diags, c = dirmult_obsinfo!(diagonals, X, α) \n",
    "        obsinfo[rowindex, rowindex] = Diagonal(diags) - c\n",
    "    \n",
    "        return loglike_new, num_iter, lastα, last∇, obsinfo\n",
    "end\n",
    "traindata = readcsv(\"optdigits.tra\", Int) \n",
    "Xd = traindata[traindata[:, end] .== 0, 1:64]'\n",
    "@time dirmult_newton(Xd; printout = true,tolfun=1e-8,maxiters = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Find the MLE for the subset of digit 0, digit 1, ..., and digit 9 separately using your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglike_loop = dirmult_logpdf(X, α) = -37813.58076528884\n",
      "loglike_loop = dirmult_logpdf(X, α) = -42575.16724071358\n",
      "loglike_loop = dirmult_logpdf(X, α) = -40257.839537528635\n",
      "loglike_loop = dirmult_logpdf(X, α) = -40733.347636875835\n",
      "loglike_loop = dirmult_logpdf(X, α) = -43832.500801433394\n",
      "loglike_loop = dirmult_logpdf(X, α) = -41523.5232388789\n",
      "loglike_loop = dirmult_logpdf(X, α) = -37837.092856415045\n",
      "loglike_loop = dirmult_logpdf(X, α) = -40563.62644771354\n",
      "loglike_loop = dirmult_logpdf(X, α) = -43298.39656251782\n",
      "loglike_loop = dirmult_logpdf(X, α) = -44057.83578034517\n",
      " 71.865560 seconds (95.61 M allocations: 30.939 GiB, 2.09% gc time)\n"
     ]
    }
   ],
   "source": [
    "traindata = readcsv(\"optdigits.tra\", Int) \n",
    "mle = zeros(64, 10)\n",
    "\n",
    "\n",
    "loglike_new = 0.0\n",
    "num_iter = 0\n",
    "lastα = zeros(64)\n",
    "last∇ = zeros(64)\n",
    "obsinfo = zeros(64, 64)\n",
    "\n",
    "@time for digit in 0:9\n",
    "    \n",
    "    Xd = traindata[traindata[:, end] .== digit, 1:64]'\n",
    "    loglike_new, num_iter, lastα, last∇, obsinfo = dirmult_newton(Xd; printout = false, tolfun=1e-8,maxiters = 200)\n",
    "    mle[:, digit + 1] = lastα\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle[:, 4] = [0.0, 0.509709, 4.60031, 7.38023, 6.86792, 2.46344, 0.218897, 0.0251735, 0.0, 1.95455, 6.80331, 4.88974, 6.15365, 5.22435, 0.518297, 0.0134401, 0.0, 0.651762, 1.29192, 1.27922, 5.92192, 4.27175, 0.249629, 0.0, 0.0, 0.0502188, 0.736951, 5.15197, 7.43089, 2.73064, 0.119619, 0.0, 0.0, 0.0233894, 0.558145, 3.51547, 6.45196, 5.93213, 1.09128, 0.0, 0.0, 0.0211571, 0.169144, 0.222963, 1.37907, 6.67517, 3.28749, 0.00510565, 0.0, 0.374606, 3.44754, 3.27551, 4.14346, 7.26971, 2.68992, 0.0205928, 0.0, 0.332903, 4.95253, 7.6872, 6.83445, 3.57867, 0.489783, 0.00232005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " 0.0       \n",
       " 0.509709  \n",
       " 4.60031   \n",
       " 7.38023   \n",
       " 6.86792   \n",
       " 2.46344   \n",
       " 0.218897  \n",
       " 0.0251735 \n",
       " 0.0       \n",
       " 1.95455   \n",
       " 6.80331   \n",
       " 4.88974   \n",
       " 6.15365   \n",
       " ⋮         \n",
       " 4.14346   \n",
       " 7.26971   \n",
       " 2.68992   \n",
       " 0.0205928 \n",
       " 0.0       \n",
       " 0.332903  \n",
       " 4.95253   \n",
       " 7.6872    \n",
       " 6.83445   \n",
       " 3.57867   \n",
       " 0.489783  \n",
       " 0.00232005"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10\n",
    "\n",
    "As $\\alpha / |\\alpha| \\to \\mathbf{p}$, the Dirichlet-multinomial distribution converges to a multinomial with parameter $\\mathbf{p}$. Therefore multinomial can be considered as a special Dirichlet-multinomial with $|\\alpha|=\\infty$. Perform a likelihood ratio test (LRT) whether Dirichlet-multinomial offers a better fit than multinomial for digits 0, 1, ..., 9 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11\n",
    "\n",
    "Now we can construct a simple Bayesian rule for handwritten digits recognition:\n",
    "$$\n",
    "\t\\mathbf{x}\t\\mapsto \\arg \\max_k \\widehat \\pi_k f(x|\\widehat \\alpha_k).\n",
    "$$\n",
    "Here we can use the proportion of digit $k$ in the training set as the prior probability $\\widehat \\pi_k$. Report the performance of your classifier on the test set of 1797 digits in `optdigits.tes`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
