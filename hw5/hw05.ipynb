{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From hw4, the log-likelihood for iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ is **\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "With $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$.\n",
    "\n",
    "$$\n",
    "\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j) = \\alpha_j (\\alpha_j + 1)\\dots(\\alpha_j + x_{ij}-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|) = |\\alpha|(|\\alpha| + 1) \\dots (|\\alpha| + |\\mathbf{x}_i| - 1)\n",
    "$$\n",
    "\n",
    "So we have \n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 2**\n",
    "Differentiate both sides wrt $\\alpha_j$, the LHS is zero\n",
    "\n",
    "$$\n",
    "0 = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d \\mathbf{p_2}_j^{\\alpha_j-1} \\frac{d}{d\\alpha_j}\\mathbf{p}_2\n",
    "-\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d \\mathbf{p_1}_j^{\\alpha_j-1} \\frac{d}{d\\alpha_j}\\mathbf{p}_1+\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{d}{d\\alpha_j}[ \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}] \\, d\\mathbf{p}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 2**\n",
    "Differentiate both sides wrt $\\alpha_j$, the LHS is zero\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\alpha_j}1 &= \\frac{d}{d\\alpha_j}\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\\\\n",
    "0 &= \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d \\mathbf{p_2}_j^{\\alpha_j-1} \\frac{d}{d\\alpha_j}\\mathbf{p}_2\n",
    "-\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d \\mathbf{p_1}_j^{\\alpha_j-1} \\frac{d}{d\\alpha_j}\\mathbf{p}_1+\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{d}{d\\alpha_j}[ \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}] \\, d\\mathbf{p}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\because \\frac{d}{d\\alpha_j}\\mathbf{p}_2=0 \\,\\frac{d}{d\\alpha_j}\\mathbf{p}_1=0\\\\\n",
    "\\therefore \\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{d}{d\\alpha_j}[ \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}] \\, d\\mathbf{p}=0 \n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    " \\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}[\\prod_{i=1}^d p_i^{\\alpha_i - 1}\\frac{\\Gamma(|\\alpha|)'}{\\prod_{i=1}^{d}\\Gamma(\\alpha_i)}-\\frac{\\Gamma(|\\alpha|)}{\\prod_{i\\neq j}^d\\Gamma(\\alpha_i)}\\frac{\\Gamma(\\alpha_j)'}{\\Gamma(\\alpha_j)}^2\\prod_{i=1}^dp_i^{\\alpha_i-1}+\\frac{\\Gamma(|\\alpha|)}{\\prod_{i= 1}^d\\Gamma(\\alpha_i)}\\prod_{i\\neq j}^d p_i^{\\alpha_i-1}\\ln(p_j) p_j^{\\alpha_j-1}]\\, d\\mathbf{p}\\, d\\mathbf{p}=0\\\\\n",
    "\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\prod_{i=1}^d p_i^{\\alpha_i - 1}\\frac{\\Gamma(|\\alpha|)'}{\\prod_{i=1}^{d}\\Gamma(\\alpha_i)}\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)}\\, d\\mathbf{p}-\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{i=1}^d\\Gamma(\\alpha_i)}\\frac{\\Gamma(\\alpha_j)'}{\\Gamma(\\alpha_j)}\\prod_{i=1}^dp_i^{\\alpha_i-1}\\, d\\mathbf{p}+\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{i= 1}^d\\Gamma(\\alpha_i)}\\prod_{i=1}^d p_i^{\\alpha_i-1}\\ln(p_j)\\, d\\mathbf{p}=0\\\\\n",
    "\\frac{\\Gamma(|\\alpha|)'}{\\Gamma(|\\alpha|)}-\\frac{\\Gamma(\\alpha_j)'}{\\Gamma(\\alpha_j)}+\\mathbf{E}(\\ln p_j)=0\\\\\n",
    "\\therefore \\mathbf{E}(\\ln p_j)=\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "    logpdf = 0.0\n",
    "    \n",
    "    t = 0.0\n",
    "    for i in 1:length(x)\n",
    "\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            return - Inf\n",
    "        elseif α[i] > 0\n",
    "            t += lgamma(α[i] + x[i]) - lgamma(α[i]) - lfact(x[i])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    if sumα == 0 && sumx == 0\n",
    "        return 0.0\n",
    "    elseif sumα > 0 && sumx == 0 \n",
    "        return - Inf\n",
    "    else \n",
    "        logpdf = t + lfact(sumx) + lgamma(sumα) - lgamma(sumx + sumα)\n",
    "    end\n",
    "    \n",
    "    return logpdf\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    \n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "    return sum(r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 2 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "using SpecialFunctions\n",
    "\n",
    "function dirmult_score(x::Vector, α::Vector)\n",
    "    \n",
    "    score = zeros(length(α))\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    for j in 1:length(x)\n",
    "        #println(\"length(x) is\", length(x))\n",
    "        score[j] = digamma(α[j] + x[j]) - digamma(α[j]) \n",
    "    end\n",
    "    \n",
    "    common_term = digamma(sumx + sumα) - digamma(sumα)\n",
    "\n",
    "    score .= score - common_term\n",
    "    return score\n",
    "end\n",
    "\n",
    "function dirmult_score!(s::Vector, X::Matrix, α::Vector)\n",
    "    d, n = size(X)\n",
    "    for i in 1:d\n",
    "        for j in 1:n\n",
    "            s[i] += dirmult_score(X[:, j], α)[i]\n",
    "        end \n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "\n",
    "function dirmult_score(X::Matrix, α::Vector)\n",
    "    s = zeros(size(X, 1))\n",
    "    dirmult_score!(s, X, α)\n",
    "    return s\n",
    "end\n",
    "\n",
    "#dirmult_score(X, α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 9.5, 486.38, 586.082, 566.828, 330.498, 16.6111, 0.0, 0.0, 189.207  …  328.915, 0.0, 0.0, 5.36111, 492.343, 590.78, 589.795, 481.73, 48.9146, 0.0], 4.935786138055547)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SpecialFunctions\n",
    "\n",
    "function dirmult_obsinfo(x::Vector, α::Vector)\n",
    "    \n",
    "    diag_matrix = zeros(length(α))\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    for i in 1:length(x)\n",
    "        diag_matrix[i] = trigamma(α[i]) - trigamma(α[i] + x[i]) \n",
    "    end\n",
    "    \n",
    "    constant_term = trigamma(sumα) - trigamma(sumx + sumα) \n",
    "    return diag_matrix, constant_term\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(obsinfo::Vector, X::Matrix, α::Vector)\n",
    "    \n",
    "    constant = 0.0\n",
    "    for j in 1:size(X, 2)\n",
    "        for i in 1:size(X, 1)\n",
    "            obsinfo[i] += dirmult_obsinfo(X[:, j], α)[1][i]\n",
    "        end \n",
    "        constant += dirmult_obsinfo(X[:, j], α)[2]\n",
    "    end\n",
    "    return obsinfo, constant\n",
    "end\n",
    "\n",
    "\n",
    "function dirmult_obsinfo(X::Matrix, α::Vector)\n",
    "    obsinfo = zeros(size(X, 1))\n",
    "    dirmult_obsinfo!(obsinfo, X, α)\n",
    "end\n",
    "\n",
    "dirmult_obsinfo(X, α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'dirmult_newton :: Tuple{Array{T,2} where T}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "function dirmult_newton(X::Matrix;\n",
    "            printout = false,\n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-6,\n",
    "            α0::Vector{Float64} = dirmult_startingvalue_mom(X))\n",
    "    \n",
    "    rowindex = find(sum(X, 2))\n",
    "    colind = find(sum(X, 1))\n",
    "    X = X[rowindex, colind] \n",
    "     \n",
    "    d, n = size(X)\n",
    "\n",
    "    \n",
    "    # set default starting point from Q7\n",
    "    α = α0[rowindex]\n",
    "    αnew = similar(α)\n",
    "    \n",
    "    # preallocation: score, obsinfo diagonals & constant, \n",
    "    ∇ = zeros(d)\n",
    "    diagonals = zeros(d)\n",
    "    invdiagonals = zeros(d)\n",
    "    \n",
    "    outerproduct = zeros(d, d)\n",
    "    newton_direction = zeros(d)\n",
    "    \n",
    "    constantC = 0.0\n",
    "    invconstantC = 0.0\n",
    "    loglike_new = 0.0\n",
    "    \n",
    "\n",
    "    loglike_old = dirmult_logpdf(X, α)\n",
    "    \n",
    "    # Newton loop\n",
    "    num_iter = 0\n",
    "    for iter in 1:maxiters\n",
    "        \n",
    "        ###############evaluate gradient (score)\n",
    "        dirmult_score!(∇, X, α) #64 element array \n",
    "        \n",
    "        # compute Newton's direction\n",
    "        ############### first get the P.D. approximation for obs info\n",
    "        ############### obs info\n",
    "        diagoals, constantC = dirmult_obsinfo!(diagonals, X, α)\n",
    "        invdiagonals .= 1.0 ./ diagoals\n",
    "        invconstantC = 1.0 / constantC\n",
    "        \n",
    "        tmp = sum(invdiagonals)\n",
    "        if invconstantC <= tmp\n",
    "            invconstantC = 1.01 * tmp\n",
    "            print(\"reset constant C\")\n",
    "        end\n",
    "        \n",
    "        # now A = D - c 11'\n",
    "        #the newton direction = A^(-1) * score \n",
    "        \n",
    "        newton_direction .= invdiagonals .* ∇ .+ \n",
    "                A_mul_Bt!(outerproduct, invdiagonals, invdiagonals) * \n",
    "                ∇ /\n",
    "                (invconstantC - tmp)\n",
    "\n",
    "        # line search loop\n",
    "        step = 1.0\n",
    "        \n",
    "        # constrain alpha to have all postive elements \n",
    "        for i in eachindex(α)       \n",
    "            if newton_direction[i] < 0\n",
    "                step = min(- α[i] / newton_direction[i] * 0.99, step)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for lsiter in 1:10\n",
    "            \n",
    "            αnew .= α .+ step .* newton_direction\n",
    "            loglike_new = dirmult_logpdf(X, αnew)\n",
    "         \n",
    "            if loglike_new > loglike_old\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            if lsiter < 10\n",
    "                step /= 2.0\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        copy!(α, αnew)\n",
    "        \n",
    "        if printout\n",
    "            println(\"iteration \", iter, \", loglike = \", loglike_new)\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(loglike_new - loglike_old) < tolfun * (abs(loglike_old) + 1)\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        loglike_old = loglike_new\n",
    "        num_iter += 1  \n",
    "    end\n",
    "    \n",
    "        lastα = zeros(length(α0))\n",
    "        lastα[rowindex] = α\n",
    "    \n",
    "        last∇ = zeros(length(α0))\n",
    "        last∇[rowindex] = dirmult_score(X, α)\n",
    "    \n",
    "        obsinfo = zeros(length(α0), length(α0))\n",
    "        diags, c = dirmult_obsinfo!(diagonals, X, α) \n",
    "        obsinfo[rowindex, rowindex] = Diagonal(diags) - c\n",
    "    \n",
    "        return loglike_new, num_iter, lastα, last∇, obsinfo\n",
    "end\n",
    "#trainset = readcsv(\"optdigits.tra\", Int) \n",
    "#X = trainset[trainset[:, end] .== 0, 1:64]\n",
    "#X = X'\n",
    "#@time dirmult_newton(X; printout = true,tolfun=1e-8,maxiters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_startingvalue_mom (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "function dirmult_startingvalue_mom(X::Matrix)\n",
    "    \n",
    "    d, n = size(X)\n",
    "    αsum_ini = ones(64)\n",
    "    \n",
    "    vec = zeros(n)\n",
    "    k = 0.0\n",
    "    \n",
    "    for j in 1:d\n",
    "        \n",
    "        vec = X[j,:]'./ sum(X, 1)\n",
    "        num = sum(vec.^2)\n",
    "        denum = sum(vec)\n",
    "    \n",
    "        if denum > 0\n",
    "            k += num / denum\n",
    "        end\n",
    "    end\n",
    "\n",
    "    αsum_ini = (d - k) / (k - 1.0)\n",
    "\n",
    "    grandsum = sum(X)\n",
    "    \n",
    "    α_ini = zeros(Float64, d)\n",
    "    \n",
    "    for i in 1:d\n",
    "        α_ini[i] = αsum_ini * sum(X, 2)[i] / grandsum\n",
    "    end\n",
    "\n",
    "    return α_ini\n",
    "end\n",
    "\n",
    "#α0 = dirmult_startingvalue_mom(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'dirmult_mm :: Tuple{AbstractArray{T,2} where T}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X0::AbstractMatrix; \n",
    "    α0::Vector = dirmult_startingvalue_mom(X0), \n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6,\n",
    "    printout = false\n",
    "    )\n",
    "    \n",
    "    rowindex = find(sum(X0, 2))\n",
    "    colind = find(sum(X0, 1))\n",
    "    X = X0[rowindex, colind] \n",
    "    d, n = size(X)\n",
    "    α = α0[rowindex]\n",
    "    αnew = similar(α)\n",
    "    \n",
    "    colsum = sum(X, 1)\n",
    "    maxi_xsum, = findmax(colsum)\n",
    "\n",
    "\n",
    "    #get r_k\n",
    "    r_k = zeros(maxi_xsum)\n",
    "    for k in 0:(maxi_xsum - 1)\n",
    "        r_k[k + 1] = count(x -> x > k, colsum) \n",
    "    end\n",
    "    \n",
    "    #get max(i) x_ij\n",
    "    max_x_ij = zeros(d)\n",
    "    s_matrix = zeros(d, maximum(X)) .- 99\n",
    "    \n",
    "    for j in 1:d\n",
    "    \n",
    "        max_x_ij[j] , = findmax(X[j,:])\n",
    "            for k in 0:(max_x_ij[j] - 1)\n",
    "                s_matrix[j, Int(k + 1)] = count(x -> x > k, X[j,:]) \n",
    "            end\n",
    "    end\n",
    "    \n",
    "    #Begin interation\n",
    "    sumα = 0.0    \n",
    "    num_iter = 0\n",
    "    loglike_new = 0.0\n",
    "    loglike_old = 0.0\n",
    "    for iter in 1:maxiters\n",
    "    \n",
    "        sumα = sum(α)\n",
    "        \n",
    "        for j in 1:d\n",
    "                \n",
    "            num = 0.0\n",
    "            denum = 1.0\n",
    "        \n",
    "            for k in 0:(max_x_ij[j] - 1)\n",
    "                \n",
    "                num += s_matrix[j, Int(k + 1)] / (α[j] + k) \n",
    "            end\n",
    "        \n",
    "            for k in 0:(maxi_xsum - 1)\n",
    "            \n",
    "                denum += r_k[Int(k + 1)] / (sumα + k) \n",
    "            end\n",
    "          \n",
    "            αnew[j] = num / denum * α[j]       \n",
    "        end\n",
    "        \n",
    "        loglike_new = dirmult_logpdf(X, αnew)\n",
    "        loglike_old = dirmult_logpdf(X, α)\n",
    "        \n",
    "        if printout \n",
    "            println(\"iteration \", iter, \", loglike = \", loglike_new)\n",
    "        end\n",
    "        \n",
    "        if abs(loglike_new - loglike_old) < tolfun * (abs(loglike_old) + 1)\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        copy!(α, αnew)     \n",
    "        num_iter += 1\n",
    "            \n",
    "    end\n",
    "    \n",
    "    # for output\n",
    "    lastα = zeros(length(α0))\n",
    "    lastα[rowindex] = α\n",
    "    \n",
    "    last∇ = zeros(length(α0))\n",
    "    last∇[rowindex] = dirmult_score(X, α)\n",
    "    \n",
    "    obsinfo = zeros(length(α0), length(α0))\n",
    "    diagonals = similar(α)\n",
    "    diags, c = dirmult_obsinfo!(diagonals, X, α) \n",
    "    obsinfo[rowindex, rowindex] = Diagonal(diags) - c\n",
    "    \n",
    "    return loglike_new, num_iter, lastα, last∇, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, loglike = -475809.81573910546\n",
      "iteration 2, loglike = -474917.0833918283\n",
      "iteration 3, loglike = -474356.61339102976\n",
      "iteration 4, loglike = -473921.534914309\n",
      "iteration 5, loglike = -473568.04663193715\n",
      "iteration 6, loglike = -473278.9923318428\n",
      "iteration 7, loglike = -473043.0724587395\n",
      "iteration 8, loglike = -472851.19799234765\n",
      "iteration 9, loglike = -472695.7185529322\n",
      "iteration 10, loglike = -472570.16902136354\n",
      "iteration 11, loglike = -472469.1141937516\n",
      "iteration 12, loglike = -472388.0155460995\n",
      "iteration 13, loglike = -472323.1083050298\n",
      "iteration 14, loglike = -472271.28851079324\n",
      "iteration 15, loglike = -472230.011117065\n",
      "iteration 16, loglike = -472197.1997042588\n",
      "iteration 17, loglike = -472171.16774875234\n",
      "iteration 18, loglike = -472150.5509015331\n",
      "iteration 19, loglike = -472134.24941560713\n",
      "iteration 20, loglike = -472121.3796927555\n",
      "iteration 21, loglike = -472111.23385976965\n",
      "iteration 22, loglike = -472103.2462986107\n",
      "iteration 23, loglike = -472096.966117157\n",
      "iteration 24, loglike = -472092.0346369828\n",
      "iteration 25, loglike = -472088.16707720875\n",
      "iteration 26, loglike = -472085.1377187576\n",
      "iteration 27, loglike = -472082.7679348836\n",
      "iteration 28, loglike = -472080.9165676958\n",
      "iteration 29, loglike = -472079.4722145597\n",
      "iteration 30, loglike = -472078.3470621216\n",
      "iteration 31, loglike = -472077.4719692819\n",
      "iteration 32, loglike = -472076.792554543\n",
      "iteration 33, loglike = -472076.26608845126\n",
      "iteration 34, loglike = -472075.85902970383\n",
      "  3.104020 seconds (3.60 M allocations: 890.214 MiB, 5.53% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-472075.85902970383, 33, [0.0, 0.0809401, 0.895971, 2.10577, 2.02126, 0.76503, 0.146432, 0.0151435, 0.000389632, 0.301568  …  0.460062, 0.027249, 0.000129825, 0.0654619, 0.920906, 2.11521, 1.94414, 0.9463, 0.231694, 0.027795], [0.0, -3.52657, -6.8137, -8.19789, -8.17018, -6.85646, -4.33885, -3.40467, -3.29505, -4.90639  …  -6.15559, -3.43041, -3.29311, -3.53608, -7.00893, -8.26513, -8.36258, -7.54628, -4.88821, -3.45653], [0.0 0.0 … 0.0 0.0; 0.0 90744.1 … -68.7405 -68.7405; … ; 0.0 -68.7405 … 25191.2 -68.7405; 0.0 -68.7405 … -68.7405 2.68045e5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = readcsv(\"optdigits.tra\", Int)\n",
    "digits = trainset[:, end]\n",
    "X = trainset[:, 1:64]'\n",
    "@time dirmult_mm(X; printout = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 2.154849537 seconds\n",
      "elapsed time: 0.853565345 seconds\n",
      "elapsed time: 3.847045509 seconds\n",
      "elapsed time: 0.20769158 seconds\n",
      "elapsed time: 3.783115738 seconds\n",
      "elapsed time: 0.357886036 seconds\n",
      "elapsed time: 3.413883119 seconds\n",
      "elapsed time: 0.517552288 seconds\n",
      "elapsed time: 12.52974187 seconds\n",
      "elapsed time: 0.380666148 seconds\n",
      "elapsed time: 12.320347589 seconds\n",
      "elapsed time: 0.383926106 seconds\n",
      "elapsed time: 3.32510189 seconds\n",
      "elapsed time: 0.437627866 seconds\n",
      "elapsed time: 4.305634198 seconds\n",
      "elapsed time: 0.367321828 seconds\n",
      "elapsed time: 4.136953551 seconds\n",
      "elapsed time: 0.468470381 seconds\n",
      "elapsed time: 7.621271835 seconds\n",
      "elapsed time: 0.388780758 seconds\n",
      " 61.811805 seconds (94.04 M allocations: 30.276 GiB, 5.12% gc time)\n",
      "loglike_newton = [-37368.3, -42191.0, -39995.5, -40528.3, -43495.8, -41202.3, -37705.4, -40311.2, -43134.1, -43715.3]\n",
      "loglike_mm = [-37384.0, -42179.2, -39987.0, -40522.9, -43489.5, -41192.4, -37707.3, -40305.8, -43134.2, -43710.8]\n"
     ]
    }
   ],
   "source": [
    "trainset = readcsv(\"optdigits.tra\", Int) \n",
    "\n",
    "\n",
    "#to store run time, number of iterations, the maximum log likleihood \n",
    "runtime_newton = zeros(10)\n",
    "runtime_mm = zeros(10)\n",
    "\n",
    "iter_newton = zeros(10)\n",
    "iter_mm = zeros(10)\n",
    "\n",
    "loglike_newton = zeros(10)\n",
    "loglike_mm = zeros(10)\n",
    "\n",
    "# preallocation\n",
    "loglike = 0.0\n",
    "num_iter = 0\n",
    "lastα = zeros(64)\n",
    "last∇ = zeros(64)\n",
    "obsinfo = zeros(64, 64)\n",
    "trainset = readcsv(\"optdigits.tra\", Int) \n",
    "\n",
    "\n",
    "@time  for digit in 0:9\n",
    "    \n",
    "    X = trainset[trainset[:, end] .== digit, 1:64]\n",
    "    X = X'\n",
    "    \n",
    "    # Newton's method\n",
    "    tic()\n",
    "    loglike, num_iter, lastα, last∇, obsinfo = dirmult_newton(X; printout = false, \n",
    "                                                        tolfun=1e-8,maxiters = 300)\n",
    "    runtime_newton[digit + 1] = toc()\n",
    "    iter_newton[digit + 1] = num_iter\n",
    "    loglike_newton[digit + 1] = loglike\n",
    "    \n",
    "    # MM method\n",
    "    tic()\n",
    "    loglike, num_iter, lastα, last∇, obsinfo = dirmult_mm(X; printout = false, \n",
    "                                                        tolfun=1e-8,maxiters = 1000)\n",
    "    runtime_mm[digit + 1] = toc()\n",
    "    iter_mm[digit + 1] = num_iter\n",
    "    loglike_mm[digit + 1] = loglike\n",
    "end\n",
    "\n",
    "@show loglike_newton;\n",
    "@show loglike_mm;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10×7 DataFrames.DataFrame\n",
      "│ Row │ digit │ loglikel_newton │ loglikel_MM │ niter_newton │ niter_MM │ time_newton │ time_MM  │\n",
      "├─────┼───────┼─────────────────┼─────────────┼──────────────┼──────────┼─────────────┼──────────┤\n",
      "│ 1   │ 0     │ -37368.3        │ -37384.0    │ 26.0         │ 337.0    │ 2.15485     │ 0.853565 │\n",
      "│ 2   │ 1     │ -42191.0        │ -42179.2    │ 34.0         │ 37.0     │ 3.84705     │ 0.207692 │\n",
      "│ 3   │ 2     │ -39995.5        │ -39987.0    │ 34.0         │ 99.0     │ 3.78312     │ 0.357886 │\n",
      "│ 4   │ 3     │ -40528.3        │ -40522.9    │ 30.0         │ 155.0    │ 3.41388     │ 0.517552 │\n",
      "│ 5   │ 4     │ -43495.8        │ -43489.5    │ 94.0         │ 91.0     │ 12.5297     │ 0.380666 │\n",
      "│ 6   │ 5     │ -41202.3        │ -41192.4    │ 107.0        │ 111.0    │ 12.3203     │ 0.383926 │\n",
      "│ 7   │ 6     │ -37705.4        │ -37707.3    │ 39.0         │ 151.0    │ 3.3251      │ 0.437628 │\n",
      "│ 8   │ 7     │ -40311.2        │ -40305.8    │ 44.0         │ 113.0    │ 4.30563     │ 0.367322 │\n",
      "│ 9   │ 8     │ -43134.1        │ -43134.2    │ 44.0         │ 153.0    │ 4.13695     │ 0.46847  │\n",
      "│ 10  │ 9     │ -43715.3        │ -43710.8    │ 71.0         │ 113.0    │ 7.62127     │ 0.388781 │"
     ]
    }
   ],
   "source": [
    "#Pkg.add(\"DataFrames\")\n",
    "using DataFrames\n",
    "result = DataFrame(digit = 0:9, \n",
    "    loglikel_newton = loglike_newton,loglikel_MM = loglike_mm,\n",
    "    niter_newton = iter_newton, niter_MM = iter_mm,\n",
    "    time_newton = runtime_newton, time_MM = runtime_mm)\n",
    "showall(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** From above it can be seen that MM algorithm runs much faster than Newton's method, though more iterations are needed. Newton's method converges faster, with respect to number of iterations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
