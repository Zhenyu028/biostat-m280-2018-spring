{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From hw4, the log-likelihood for iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ is **\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "With $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$.\n",
    "\n",
    "$$\n",
    "\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j) = \\alpha_j (\\alpha_j + 1)\\dots(\\alpha_j + x_{ij}-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|) = |\\alpha|(|\\alpha| + 1) \\dots (|\\alpha| + |\\mathbf{x}_i| - 1)\n",
    "$$\n",
    "\n",
    "So we have \n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "    logpdf = 0.0\n",
    "    \n",
    "    t = 0.0\n",
    "    for i in 1:length(x)\n",
    "\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            return - Inf\n",
    "        elseif α[i] > 0\n",
    "            t += lgamma(α[i] + x[i]) - lgamma(α[i]) - lfact(x[i])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    if sumα == 0 && sumx == 0\n",
    "        return 0.0\n",
    "    elseif sumα > 0 && sumx == 0 \n",
    "        return - Inf\n",
    "    else \n",
    "        logpdf = t + lfact(sumx) + lgamma(sumα) - lgamma(sumx + sumα)\n",
    "    end\n",
    "    \n",
    "    return logpdf\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    \n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "    return sum(r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "using SpecialFunctions\n",
    "\n",
    "function dirmult_score(x::Vector, α::Vector)\n",
    "    \n",
    "    score = zeros(length(α))\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    for j in 1:length(x)\n",
    "        #println(\"length(x) is\", length(x))\n",
    "        score[j] = digamma(α[j] + x[j]) - digamma(α[j]) \n",
    "    end\n",
    "    \n",
    "    common_term = digamma(sumx + sumα) - digamma(sumα)\n",
    "\n",
    "    score .= score - common_term\n",
    "    return score\n",
    "end\n",
    "\n",
    "function dirmult_score!(s::Vector, X::Matrix, α::Vector)\n",
    "    d, n = size(X)\n",
    "    for i in 1:d\n",
    "        for j in 1:n\n",
    "            s[i] += dirmult_score(X[:, j], α)[i]\n",
    "        end \n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "\n",
    "function dirmult_score(X::Matrix, α::Vector)\n",
    "    s = zeros(size(X, 1))\n",
    "    dirmult_score!(s, X, α)\n",
    "    return s\n",
    "end\n",
    "\n",
    "#dirmult_score(X, α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "function dirmult_newton(X::Matrix;\n",
    "            printout = false,\n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-6,\n",
    "            α0::Vector{Float64} = dirmult_startingvalue_mom(X))\n",
    "    \n",
    "    rowindex = find(sum(X, 2))\n",
    "    colind = find(sum(X, 1))\n",
    "    X = X[rowindex, colind] \n",
    "     \n",
    "    d, n = size(X)\n",
    "\n",
    "    \n",
    "    # set default starting point from Q7\n",
    "    α = α0[rowindex]\n",
    "    αnew = similar(α)\n",
    "    \n",
    "    # preallocation: score, obsinfo diagonals & constant, \n",
    "    ∇ = zeros(d)\n",
    "    diagonals = zeros(d)\n",
    "    invdiagonals = zeros(d)\n",
    "    \n",
    "    outerproduct = zeros(d, d)\n",
    "    newton_direction = zeros(d)\n",
    "    \n",
    "    constantC = 0.0\n",
    "    invconstantC = 0.0\n",
    "    loglike_new = 0.0\n",
    "    \n",
    "\n",
    "    loglike_old = dirmult_logpdf(X, α)\n",
    "    \n",
    "    # Newton loop\n",
    "    num_iter = 0\n",
    "    for iter in 1:maxiters\n",
    "        \n",
    "        ###############evaluate gradient (score)\n",
    "        dirmult_score!(∇, X, α) #64 element array \n",
    "        \n",
    "        # compute Newton's direction\n",
    "        ############### first get the P.D. approximation for obs info\n",
    "        ############### obs info\n",
    "        diagoals, constantC = dirmult_obsinfo!(diagonals, X, α)\n",
    "        invdiagonals .= 1.0 ./ diagoals\n",
    "        invconstantC = 1.0 / constantC\n",
    "        \n",
    "        tmp = sum(invdiagonals)\n",
    "        if invconstantC <= tmp\n",
    "            invconstantC = 1.01 * tmp\n",
    "            print(\"reset constant C\")\n",
    "        end\n",
    "        \n",
    "        # now A = D - c 11'\n",
    "        #the newton direction = A^(-1) * score \n",
    "        \n",
    "        newton_direction .= invdiagonals .* ∇ .+ \n",
    "                A_mul_Bt!(outerproduct, invdiagonals, invdiagonals) * \n",
    "                ∇ /\n",
    "                (invconstantC - tmp)\n",
    "\n",
    "        # line search loop\n",
    "        step = 1.0\n",
    "        \n",
    "        # constrain alpha to have all postive elements \n",
    "        for i in eachindex(α)       \n",
    "            if newton_direction[i] < 0\n",
    "                step = min(- α[i] / newton_direction[i] * 0.99, step)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for lsiter in 1:10\n",
    "            \n",
    "            αnew .= α .+ step .* newton_direction\n",
    "            loglike_new = dirmult_logpdf(X, αnew)\n",
    "         \n",
    "            if loglike_new > loglike_old\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            if lsiter < 10\n",
    "                step /= 2.0\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        copy!(α, αnew)\n",
    "        \n",
    "        if printout\n",
    "            println(\"iteration \", iter, \", loglike = \", loglike_new)\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(loglike_new - loglike_old) < tolfun * (abs(loglike_old) + 1)\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        loglike_old = loglike_new\n",
    "        num_iter += 1  \n",
    "    end\n",
    "    \n",
    "        lastα = zeros(length(α0))\n",
    "        lastα[rowindex] = α\n",
    "    \n",
    "        last∇ = zeros(length(α0))\n",
    "        last∇[rowindex] = dirmult_score(X, α)\n",
    "    \n",
    "        obsinfo = zeros(length(α0), length(α0))\n",
    "        diags, c = dirmult_obsinfo!(diagonals, X, α) \n",
    "        obsinfo[rowindex, rowindex] = Diagonal(diags) - c\n",
    "    \n",
    "        return loglike_new, num_iter, lastα, last∇, obsinfo\n",
    "end\n",
    "#trainset = readcsv(\"optdigits.tra\", Int) \n",
    "#X = trainset[trainset[:, end] .== 0, 1:64]\n",
    "#X = X'\n",
    "#@time dirmult_newton(X; printout = true,tolfun=1e-8,maxiters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_startingvalue_mom (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "function dirmult_startingvalue_mom(X::Matrix)\n",
    "    \n",
    "    d, n = size(X)\n",
    "    αsum_ini = ones(64)\n",
    "    \n",
    "    vec = zeros(n)\n",
    "    k = 0.0\n",
    "    \n",
    "    for j in 1:d\n",
    "        \n",
    "        vec = X[j,:]'./ sum(X, 1)\n",
    "        num = sum(vec.^2)\n",
    "        denum = sum(vec)\n",
    "    \n",
    "        if denum > 0\n",
    "            k += num / denum\n",
    "        end\n",
    "    end\n",
    "\n",
    "    αsum_ini = (d - k) / (k - 1.0)\n",
    "\n",
    "    grandsum = sum(X)\n",
    "    \n",
    "    α_ini = zeros(Float64, d)\n",
    "    \n",
    "    for i in 1:d\n",
    "        α_ini[i] = αsum_ini * sum(X, 2)[i] / grandsum\n",
    "    end\n",
    "\n",
    "    return α_ini\n",
    "end\n",
    "\n",
    "#α0 = dirmult_startingvalue_mom(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X::AbstractMatrix; \n",
    "    α0::Vector = dirmult_mom(X), \n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6\n",
    "    )\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    # for output\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
