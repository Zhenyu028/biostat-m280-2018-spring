{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From hw4, the log-likelihood for iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ is **\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "With $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$.\n",
    "\n",
    "$$\n",
    "\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j) = \\alpha_j (\\alpha_j + 1)\\dots(\\alpha_j + x_{ij}-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|) = |\\alpha|(|\\alpha| + 1) \\dots (|\\alpha| + |\\mathbf{x}_i| - 1)\n",
    "$$\n",
    "\n",
    "So we have \n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 2**\n",
    "Differentiate both sides wrt $\\alpha_j$, the LHS is zero\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "0 & = \\int_{\\Delta_d} \\frac{d}{d\\alpha_j} \\left( \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}  \\right) \\prod_{j=1}^d p_j^{\\alpha_j-1}  + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}  \\frac{d}{d\\alpha_j} \\left( \\prod_{j=1}^d p_j^{\\alpha_j-1}\\right) \\, d\\mathbf{p}. \\\\\n",
    "& = \\int_{\\Delta_d}  \\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j) - \\Gamma(|\\alpha|) \\Gamma(\\alpha_j) \\prod_{j' \\neq j}^d  \\Gamma(\\alpha_{j'})}{\\left( \\prod_{j=1}^d \\Gamma(\\alpha_j) \\right)^2} \\prod_{j=1}^d p_j^{\\alpha_j-1}  + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}  ln p_j \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\\\\\n",
    "& = \\int_{\\Delta_d}  \\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j) - \\Gamma(|\\alpha|) \\Gamma(\\alpha_j) \\prod_{j' \\neq j}^d  \\Gamma(\\alpha_{j'})}{\\left( \\prod_{j=1}^d \\Gamma(\\alpha_j) \\right)^2} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}  + \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}  ln p_j \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We recognize the second part is just $\\mathbf{E}(\\ln P_j)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{E}(\\ln P_j) & = - \\int_{\\Delta_d}  \\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j) - \\Gamma(|\\alpha|) \\Gamma(\\alpha_j) \\prod_{j' \\neq j}^d  \\Gamma(\\alpha_{j'})}{\\left( \\prod_{j=1}^d \\Gamma(\\alpha_j) \\right)^2} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\\\\n",
    "& =  \\int_{\\Delta_d}  \\frac{\\Gamma(|\\alpha|) \\Gamma(\\alpha_j) \\prod_{j' \\neq j}^d  \\Gamma(\\alpha_{j'}) - \\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\left( \\prod_{j=1}^d \\Gamma(\\alpha_j) \\right)^2} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} \\\\\n",
    "& =  \\int_{\\Delta_d}  \\frac{\\Gamma(|\\alpha|) \\Gamma(\\alpha_j) \\prod_{j' \\neq j}^d  \\Gamma(\\alpha_{j'}) - \\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\left( \\prod_{j=1}^d \\Gamma(\\alpha_j) \\right)^2} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} \\\\\n",
    "& = \\frac{\\Gamma(|\\alpha|) \\Gamma(\\alpha_j) \\prod_{j' \\neq j}^d  \\Gamma(\\alpha_{j'}) - \\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\left( \\prod_{j=1}^d \\Gamma(\\alpha_j) \\right)^2} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)} \\int_{\\Delta_d}    \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} \\\\\n",
    "& = \\frac{\\Gamma(|\\alpha|) \\Gamma(\\alpha_j) \\prod_{j' \\neq j}^d  \\Gamma(\\alpha_{j'}) - \\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\left( \\prod_{j=1}^d \\Gamma(\\alpha_j) \\right)^2} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)}\\\\\n",
    "& = \\Psi(\\alpha_j) - \\Psi(|\\alpha|)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The key is that $\\prod_{j=1}^d p_j^{\\alpha_j-1}$ is the kernel for Dirichlet distribution so we can complete the integral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **question 3**\n",
    " \n",
    " We treat $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and denote the joint density of complete data is $\\prod_{i = 1}^n f(\\mathbf{x}_i, \\mathbf{p}_i, \\alpha)$ \n",
    " $$\n",
    " Q(\\alpha|\\alpha^{(t)}) = \\mathbf{E}\\left( ln \\prod_{i = 1}^n f(\\mathbf{x}_i, \\mathbf{p}_i, \\mathbf{\\alpha})| \\mathbf{x}_1, \\dots, \\mathbf{x}_n, \\mathbf{\\alpha}^{(t)}\\right)\n",
    " $$\n",
    " The expectation is wrt to the conditional distribution \n",
    " $$f(\\mathbf{p}_1, \\dots, \\mathbf{p}_n | \\mathbf{x}_1,\\dots,\\mathbf{x}_n,\\alpha^{(t)})$$\n",
    " \n",
    " Since ecpectation is a linear operation, we have $Q(\\alpha|\\alpha^{(t)}) = \\sum_{i = 1}^n Q_i (\\alpha|\\alpha^{(t)})$, with $Q_i (\\alpha|\\alpha^{(t)}) = \\mathbf{E}[lnf(\\mathbf{x}_i, \\mathbf{p}_i, \\mathbf{\\alpha})| \\mathbf{x}_1, \\dots, \\mathbf{x}_n, \\mathbf{\\alpha}^{(t)})]$, and the expectation is wrt to conditional distribution $f(\\mathbf{p}_i|\\mathbf{x}_1, \\dots, \\mathbf(x)_n, \\alpha^{(t)})$\n",
    " \n",
    " The multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ are independent Dirichlet distributions \n",
    "\n",
    "$$\n",
    "f(\\mathbf{p}_i|\\mathbf{x}_1,\\dots,\\mathbf{x}_n,\\mathbf\\alpha^{(t)})=\\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x}_i|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1}\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "Q_i (\\alpha|\\alpha^{(t)}) = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x}_i|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1} \\times \\left( ln {m_i\\choose x_i} + \\sum_{j = 1}^d (x_{ij} + \\alpha_j - 1)lnp_{ij} +ln \\Gamma(|\\mathbf{\\alpha}|)\\right) d\\mathbf{p}_i\n",
    "$$\n",
    "Put $\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|)$ we can simplify integrals and get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(\\alpha|\\alpha^{(t)}) &=  \\sum_{i=1}^n ln {m_i \\choose x_i} + \\sum_{i = 1} ^n \\sum_{j=1}^d (x_{ij}+\\alpha_j -1)[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)] + nln \\Gamma(|\\alpha|) -n \\sum_{j = 1}^d ln \\Gamma(\\alpha_j)\\\\ & = \\sum_{j=1}^d \\sum_{i = 1} ^n [\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)] - n \\sum_{j=1}^d ln \\Gamma(\\alpha_j) + nln \\Gamma(|\\alpha|) + c^t\n",
    "\\end{align}\n",
    "$$\n",
    "$c^{(t)}$ is a constant irrelevant to optimization.\n",
    "\n",
    "The Q function is not easy to optimize because $\\alpha_j$s are intertwined in the $ln\\Gamma(|\\alpha|)$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "\n",
    "    logpdf = 0.0\n",
    "    \n",
    "    t = 0.0\n",
    "    for i in 1:length(x)\n",
    "\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            return - Inf\n",
    "        elseif α[i] > 0\n",
    "            t += lgamma(α[i] + x[i]) - lgamma(α[i]) - lfact(x[i])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    if sumα == 0 && sumx == 0\n",
    "        return 0.0\n",
    "    elseif sumα > 0 && sumx == 0 \n",
    "        return - Inf\n",
    "    else \n",
    "        logpdf = t + lfact(sumx) + lgamma(sumα) - lgamma(sumx + sumα)\n",
    "    end\n",
    "    \n",
    "    return logpdf\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    \n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "    return sum(r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "using SpecialFunctions\n",
    "\n",
    "function dirmult_score(x::Vector, α::Vector)\n",
    "    \n",
    "    score = zeros(length(α))\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    for j in 1:length(x)\n",
    "        #println(\"length(x) is\", length(x))\n",
    "        score[j] = digamma(α[j] + x[j]) - digamma(α[j]) \n",
    "    end\n",
    "    \n",
    "    common_term = digamma(sumx + sumα) - digamma(sumα)\n",
    "\n",
    "    score .= score - common_term\n",
    "    return score\n",
    "end\n",
    "\n",
    "function dirmult_score!(s::Vector, X::Matrix, α::Vector)\n",
    "    d, n = size(X)\n",
    "    for i in 1:d\n",
    "        for j in 1:n\n",
    "            s[i] += dirmult_score(X[:, j], α)[i]\n",
    "        end \n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "\n",
    "function dirmult_score(X::Matrix, α::Vector)\n",
    "    s = zeros(size(X, 1))\n",
    "    dirmult_score!(s, X, α)\n",
    "    return s\n",
    "end\n",
    "\n",
    "#dirmult_score(X, α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: X not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: X not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "using SpecialFunctions\n",
    "\n",
    "function dirmult_obsinfo(x::Vector, α::Vector)\n",
    "    \n",
    "    diag_matrix = zeros(length(α))\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    for i in 1:length(x)\n",
    "        diag_matrix[i] = trigamma(α[i]) - trigamma(α[i] + x[i]) \n",
    "    end\n",
    "    \n",
    "    constant_term = trigamma(sumα) - trigamma(sumx + sumα) \n",
    "    return diag_matrix, constant_term\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(obsinfo::Vector, X::Matrix, α::Vector)\n",
    "    \n",
    "    constant = 0.0\n",
    "    for j in 1:size(X, 2)\n",
    "        for i in 1:size(X, 1)\n",
    "            obsinfo[i] += dirmult_obsinfo(X[:, j], α)[1][i]\n",
    "        end \n",
    "        constant += dirmult_obsinfo(X[:, j], α)[2]\n",
    "    end\n",
    "    return obsinfo, constant\n",
    "end\n",
    "\n",
    "\n",
    "function dirmult_obsinfo(X::Matrix, α::Vector)\n",
    "    obsinfo = zeros(size(X, 1))\n",
    "    dirmult_obsinfo!(obsinfo, X, α)\n",
    "end\n",
    "\n",
    "dirmult_obsinfo(X, α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "function dirmult_newton(X::Matrix;\n",
    "            printout = false,\n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-6,\n",
    "            α0::Vector{Float64} = dirmult_startingvalue_mom(X))\n",
    "    \n",
    "    rowindex = find(sum(X, 2))\n",
    "    colind = find(sum(X, 1))\n",
    "    X = X[rowindex, colind] \n",
    "     \n",
    "    d, n = size(X)\n",
    "\n",
    "    \n",
    "    # set default starting point from Q7\n",
    "    α = α0[rowindex]\n",
    "    αnew = similar(α)\n",
    "    \n",
    "    # preallocation: score, obsinfo diagonals & constant, \n",
    "    ∇ = zeros(d)\n",
    "    diagonals = zeros(d)\n",
    "    invdiagonals = zeros(d)\n",
    "    \n",
    "    outerproduct = zeros(d, d)\n",
    "    newton_direction = zeros(d)\n",
    "    \n",
    "    constantC = 0.0\n",
    "    invconstantC = 0.0\n",
    "    loglike_new = 0.0\n",
    "    \n",
    "\n",
    "    loglike_old = dirmult_logpdf(X, α)\n",
    "    \n",
    "    # Newton loop\n",
    "    num_iter = 0\n",
    "    for iter in 1:maxiters\n",
    "        \n",
    "        ###############evaluate gradient (score)\n",
    "        dirmult_score!(∇, X, α) #64 element array \n",
    "        \n",
    "        # compute Newton's direction\n",
    "        ############### first get the P.D. approximation for obs info\n",
    "        ############### obs info\n",
    "        diagoals, constantC = dirmult_obsinfo!(diagonals, X, α)\n",
    "        invdiagonals .= 1.0 ./ diagoals\n",
    "        invconstantC = 1.0 / constantC\n",
    "        \n",
    "        tmp = sum(invdiagonals)\n",
    "        if invconstantC <= tmp\n",
    "            invconstantC = 1.01 * tmp\n",
    "            print(\"reset constant C\")\n",
    "        end\n",
    "        \n",
    "        # now A = D - c 11'\n",
    "        #the newton direction = A^(-1) * score \n",
    "        \n",
    "        newton_direction .= invdiagonals .* ∇ .+ \n",
    "                A_mul_Bt!(outerproduct, invdiagonals, invdiagonals) * \n",
    "                ∇ /\n",
    "                (invconstantC - tmp)\n",
    "\n",
    "        # line search loop\n",
    "        step = 1.0\n",
    "        \n",
    "        # constrain alpha to have all postive elements \n",
    "        for i in eachindex(α)       \n",
    "            if newton_direction[i] < 0\n",
    "                step = min(- α[i] / newton_direction[i] * 0.99, step)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for lsiter in 1:10\n",
    "            \n",
    "            αnew .= α .+ step .* newton_direction\n",
    "            loglike_new = dirmult_logpdf(X, αnew)\n",
    "         \n",
    "            if loglike_new > loglike_old\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            if lsiter < 10\n",
    "                step /= 2.0\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        copy!(α, αnew)\n",
    "        \n",
    "        if printout\n",
    "            println(\"iteration \", iter, \", loglike = \", loglike_new)\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(loglike_new - loglike_old) < tolfun * (abs(loglike_old) + 1)\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        loglike_old = loglike_new\n",
    "        num_iter += 1  \n",
    "    end\n",
    "    \n",
    "        lastα = zeros(length(α0))\n",
    "        lastα[rowindex] = α\n",
    "    \n",
    "        last∇ = zeros(length(α0))\n",
    "        last∇[rowindex] = dirmult_score(X, α)\n",
    "    \n",
    "        obsinfo = zeros(length(α0), length(α0))\n",
    "        diags, c = dirmult_obsinfo!(diagonals, X, α) \n",
    "        obsinfo[rowindex, rowindex] = Diagonal(diags) - c\n",
    "    \n",
    "        return loglike_new, num_iter, lastα, last∇, obsinfo\n",
    "end\n",
    "#trainset = readcsv(\"optdigits.tra\", Int) \n",
    "#X = trainset[trainset[:, end] .== 0, 1:64]\n",
    "#X = X'\n",
    "#@time dirmult_newton(X; printout = true,tolfun=1e-8,maxiters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_startingvalue_mom (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions\")\n",
    "function dirmult_startingvalue_mom(X::Matrix)\n",
    "    \n",
    "    d, n = size(X)\n",
    "    αsum_ini = ones(64)\n",
    "    \n",
    "    vec = zeros(n)\n",
    "    k = 0.0\n",
    "    \n",
    "    for j in 1:d\n",
    "        \n",
    "        vec = X[j,:]'./ sum(X, 1)\n",
    "        num = sum(vec.^2)\n",
    "        denum = sum(vec)\n",
    "    \n",
    "        if denum > 0\n",
    "            k += num / denum\n",
    "        end\n",
    "    end\n",
    "\n",
    "    αsum_ini = (d - k) / (k - 1.0)\n",
    "\n",
    "    grandsum = sum(X)\n",
    "    \n",
    "    α_ini = zeros(Float64, d)\n",
    "    \n",
    "    for i in 1:d\n",
    "        α_ini[i] = αsum_ini * sum(X, 2)[i] / grandsum\n",
    "    end\n",
    "\n",
    "    return α_ini\n",
    "end\n",
    "\n",
    "#α0 = dirmult_startingvalue_mom(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X0::AbstractMatrix; \n",
    "    α0::Vector = dirmult_startingvalue_mom(X0), \n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6,\n",
    "    printout = false\n",
    "    )\n",
    "    \n",
    "    rowindex = find(sum(X0, 2))\n",
    "    colind = find(sum(X0, 1))\n",
    "    X = X0[rowindex, colind] \n",
    "    d, n = size(X)\n",
    "    α = α0[rowindex]\n",
    "    αnew = similar(α)\n",
    "    \n",
    "    colsum = sum(X, 1)\n",
    "    maxi_xsum, = findmax(colsum)\n",
    "\n",
    "\n",
    "    #get r_k\n",
    "    r_k = zeros(maxi_xsum)\n",
    "    for k in 0:(maxi_xsum - 1)\n",
    "        r_k[k + 1] = count(x -> x > k, colsum) \n",
    "    end\n",
    "    \n",
    "    #get max(i) x_ij\n",
    "    max_x_ij = zeros(d)\n",
    "    s_matrix = zeros(d, maximum(X)) .- 99\n",
    "    \n",
    "    for j in 1:d\n",
    "    \n",
    "        max_x_ij[j] , = findmax(X[j,:])\n",
    "            for k in 0:(max_x_ij[j] - 1)\n",
    "                s_matrix[j, Int(k + 1)] = count(x -> x > k, X[j,:]) \n",
    "            end\n",
    "    end\n",
    "    \n",
    "    #Begin interation\n",
    "    sumα = 0.0    \n",
    "    num_iter = 0\n",
    "    loglike_new = 0.0\n",
    "    loglike_old = 0.0\n",
    "    for iter in 1:maxiters\n",
    "    \n",
    "        sumα = sum(α)\n",
    "        \n",
    "        for j in 1:d\n",
    "                \n",
    "            num = 0.0\n",
    "            denum = 1.0\n",
    "        \n",
    "            for k in 0:(max_x_ij[j] - 1)\n",
    "                \n",
    "                num += s_matrix[j, Int(k + 1)] / (α[j] + k) \n",
    "            end\n",
    "        \n",
    "            for k in 0:(maxi_xsum - 1)\n",
    "            \n",
    "                denum += r_k[Int(k + 1)] / (sumα + k) \n",
    "            end\n",
    "          \n",
    "            αnew[j] = num / denum * α[j]       \n",
    "        end\n",
    "        \n",
    "        loglike_new = dirmult_logpdf(X, αnew)\n",
    "        loglike_old = dirmult_logpdf(X, α)\n",
    "        \n",
    "        if printout \n",
    "            println(\"iteration \", iter, \", loglike = \", loglike_new)\n",
    "        end\n",
    "        \n",
    "        if abs(loglike_new - loglike_old) < tolfun * (abs(loglike_old) + 1)\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        copy!(α, αnew)     \n",
    "        num_iter += 1\n",
    "            \n",
    "    end\n",
    "    \n",
    "    # for output\n",
    "    lastα = zeros(length(α0))\n",
    "    lastα[rowindex] = α\n",
    "    \n",
    "    last∇ = zeros(length(α0))\n",
    "    last∇[rowindex] = dirmult_score(X, α)\n",
    "    \n",
    "    obsinfo = zeros(length(α0), length(α0))\n",
    "    diagonals = similar(α)\n",
    "    diags, c = dirmult_obsinfo!(diagonals, X, α) \n",
    "    obsinfo[rowindex, rowindex] = Diagonal(diags) - c\n",
    "    \n",
    "    return loglike_new, num_iter, lastα, last∇, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, loglike = -475809.81573910546\n",
      "iteration 2, loglike = -474917.0833918283\n",
      "iteration 3, loglike = -474356.61339102976\n",
      "iteration 4, loglike = -473921.534914309\n",
      "iteration 5, loglike = -473568.04663193715\n",
      "iteration 6, loglike = -473278.9923318428\n",
      "iteration 7, loglike = -473043.0724587395\n",
      "iteration 8, loglike = -472851.19799234765\n",
      "iteration 9, loglike = -472695.7185529322\n",
      "iteration 10, loglike = -472570.16902136354\n",
      "iteration 11, loglike = -472469.1141937516\n",
      "iteration 12, loglike = -472388.0155460995\n",
      "iteration 13, loglike = -472323.1083050298\n",
      "iteration 14, loglike = -472271.28851079324\n",
      "iteration 15, loglike = -472230.011117065\n",
      "iteration 16, loglike = -472197.1997042588\n",
      "iteration 17, loglike = -472171.16774875234\n",
      "iteration 18, loglike = -472150.5509015331\n",
      "iteration 19, loglike = -472134.24941560713\n",
      "iteration 20, loglike = -472121.3796927555\n",
      "iteration 21, loglike = -472111.23385976965\n",
      "iteration 22, loglike = -472103.2462986107\n",
      "iteration 23, loglike = -472096.966117157\n",
      "iteration 24, loglike = -472092.0346369828\n",
      "iteration 25, loglike = -472088.16707720875\n",
      "iteration 26, loglike = -472085.1377187576\n",
      "iteration 27, loglike = -472082.7679348836\n",
      "iteration 28, loglike = -472080.9165676958\n",
      "iteration 29, loglike = -472079.4722145597\n",
      "iteration 30, loglike = -472078.3470621216\n",
      "iteration 31, loglike = -472077.4719692819\n",
      "iteration 32, loglike = -472076.792554543\n",
      "iteration 33, loglike = -472076.26608845126\n",
      "iteration 34, loglike = -472075.85902970383\n",
      "  5.041751 seconds (4.28 M allocations: 926.294 MiB, 1.20% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-472075.85902970383, 33, [0.0, 0.0809401, 0.895971, 2.10577, 2.02126, 0.76503, 0.146432, 0.0151435, 0.000389632, 0.301568  …  0.460062, 0.027249, 0.000129825, 0.0654619, 0.920906, 2.11521, 1.94414, 0.9463, 0.231694, 0.027795], [0.0, -3.52657, -6.8137, -8.19789, -8.17018, -6.85646, -4.33885, -3.40467, -3.29505, -4.90639  …  -6.15559, -3.43041, -3.29311, -3.53608, -7.00893, -8.26513, -8.36258, -7.54628, -4.88821, -3.45653], [0.0 0.0 … 0.0 0.0; 0.0 90744.1 … -68.7405 -68.7405; … ; 0.0 -68.7405 … 25191.2 -68.7405; 0.0 -68.7405 … -68.7405 2.68045e5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = readcsv(\"optdigits.tra\", Int)\n",
    "digits = trainset[:, end]\n",
    "X = trainset[:, 1:64]'\n",
    "@time dirmult_mm(X; printout = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 4.604152018 seconds\n",
      "elapsed time: 1.202267017 seconds\n",
      "elapsed time: 6.98700946 seconds\n",
      "elapsed time: 0.265561208 seconds\n",
      "elapsed time: 5.642357037 seconds\n",
      "elapsed time: 0.461636573 seconds\n",
      "elapsed time: 5.620185434 seconds\n",
      "elapsed time: 0.670973589 seconds\n",
      "elapsed time: 18.090953289 seconds\n",
      "elapsed time: 0.48402531 seconds\n",
      "elapsed time: 16.793149988 seconds\n",
      "elapsed time: 0.553079273 seconds\n",
      "elapsed time: 4.849992413 seconds\n",
      "elapsed time: 0.574305751 seconds\n",
      "elapsed time: 6.736953334 seconds\n",
      "elapsed time: 0.484258296 seconds\n",
      "elapsed time: 6.369376983 seconds\n",
      "elapsed time: 0.650198325 seconds\n",
      "elapsed time: 11.648226992 seconds\n",
      "elapsed time: 0.748841891 seconds\n",
      " 94.972033 seconds (95.36 M allocations: 30.341 GiB, 7.77% gc time)\n",
      "loglike_newton = [-37368.3, -42191.0, -39995.5, -40528.3, -43495.8, -41202.3, -37705.4, -40311.2, -43134.1, -43715.3]\n",
      "loglike_mm = [-37384.0, -42179.2, -39987.0, -40522.9, -43489.5, -41192.4, -37707.3, -40305.8, -43134.2, -43710.8]\n"
     ]
    }
   ],
   "source": [
    "trainset = readcsv(\"optdigits.tra\", Int) \n",
    "\n",
    "\n",
    "#to store run time, number of iterations, the maximum log likleihood \n",
    "runtime_newton = zeros(10)\n",
    "runtime_mm = zeros(10)\n",
    "\n",
    "iter_newton = zeros(10)\n",
    "iter_mm = zeros(10)\n",
    "\n",
    "loglike_newton = zeros(10)\n",
    "loglike_mm = zeros(10)\n",
    "\n",
    "# preallocation\n",
    "loglike = 0.0\n",
    "num_iter = 0\n",
    "lastα = zeros(64)\n",
    "last∇ = zeros(64)\n",
    "obsinfo = zeros(64, 64)\n",
    "trainset = readcsv(\"optdigits.tra\", Int) \n",
    "\n",
    "\n",
    "@time  for digit in 0:9\n",
    "    \n",
    "    X = trainset[trainset[:, end] .== digit, 1:64]\n",
    "    X = X'\n",
    "    \n",
    "    # Newton's method\n",
    "    tic()\n",
    "    loglike, num_iter, lastα, last∇, obsinfo = \n",
    "    dirmult_newton(X; printout = false, tolfun=1e-8,maxiters = 300)\n",
    "    runtime_newton[digit + 1] = toc()\n",
    "    iter_newton[digit + 1] = num_iter\n",
    "    loglike_newton[digit + 1] = loglike\n",
    "    \n",
    "    # MM method\n",
    "    tic()\n",
    "    loglike, num_iter, lastα, last∇, obsinfo = \n",
    "    dirmult_mm(X; printout = false, tolfun=1e-8,maxiters = 1000)\n",
    "    runtime_mm[digit + 1] = toc()\n",
    "    iter_mm[digit + 1] = num_iter\n",
    "    loglike_mm[digit + 1] = loglike\n",
    "end\n",
    "\n",
    "@show loglike_newton;\n",
    "@show loglike_mm;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10×7 DataFrames.DataFrame\n",
      "│ Row │ digit │ loglikel_newton │ loglikel_MM │ niter_newton │ niter_MM │ time_newton │ time_MM  │\n",
      "├─────┼───────┼─────────────────┼─────────────┼──────────────┼──────────┼─────────────┼──────────┤\n",
      "│ 1   │ 0     │ -37368.3        │ -37384.0    │ 26.0         │ 337.0    │ 4.60415     │ 1.20227  │\n",
      "│ 2   │ 1     │ -42191.0        │ -42179.2    │ 34.0         │ 37.0     │ 6.98701     │ 0.265561 │\n",
      "│ 3   │ 2     │ -39995.5        │ -39987.0    │ 34.0         │ 99.0     │ 5.64236     │ 0.461637 │\n",
      "│ 4   │ 3     │ -40528.3        │ -40522.9    │ 30.0         │ 155.0    │ 5.62019     │ 0.670974 │\n",
      "│ 5   │ 4     │ -43495.8        │ -43489.5    │ 94.0         │ 91.0     │ 18.091      │ 0.484025 │\n",
      "│ 6   │ 5     │ -41202.3        │ -41192.4    │ 107.0        │ 111.0    │ 16.7931     │ 0.553079 │\n",
      "│ 7   │ 6     │ -37705.4        │ -37707.3    │ 39.0         │ 151.0    │ 4.84999     │ 0.574306 │\n",
      "│ 8   │ 7     │ -40311.2        │ -40305.8    │ 44.0         │ 113.0    │ 6.73695     │ 0.484258 │\n",
      "│ 9   │ 8     │ -43134.1        │ -43134.2    │ 44.0         │ 153.0    │ 6.36938     │ 0.650198 │\n",
      "│ 10  │ 9     │ -43715.3        │ -43710.8    │ 71.0         │ 113.0    │ 11.6482     │ 0.748842 │"
     ]
    }
   ],
   "source": [
    "#Pkg.add(\"DataFrames\")\n",
    "using DataFrames\n",
    "result = DataFrame(digit = 0:9, \n",
    "    loglikel_newton = loglike_newton,loglikel_MM = loglike_mm,\n",
    "    niter_newton = iter_newton, niter_MM = iter_mm,\n",
    "    time_newton = runtime_newton, time_MM = runtime_mm)\n",
    "showall(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** From above it can be seen that MM algorithm runs much faster than Newton's method, though more iterations are needed. Newton's method converges faster, with respect to number of iterations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 7**\n",
    "\n",
    "We got that for EM, the Q function is\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "\n",
    "By the supporting hyperplane inequality ($h(y) \\geq h(x) + \\nabla h(x)^T(y-x)$) for convex function, ($ln\\Gamma $ is convex) \n",
    "$$\\ln \\Gamma(|\\alpha|) \\geq \\ln \\Gamma(|\\alpha^{(t)}|) + \\Psi(|\\alpha^{(t)}|) (|\\alpha| - |\\alpha^{(t)}|)$$\n",
    "So the inequality goes\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) \\geq \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) +n[\\ln \\Gamma(|\\alpha^{(t)}|) + \\Psi(|\\alpha^{(t)}|) (|\\alpha| - |\\alpha^{(t)}|)]+ c^{(t)}\n",
    "$$\n",
    "In this new minorizing function, all $\\alpha_j$ are separated thus can be optimized independently.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
