{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 3\n",
    "\n",
    "**Due Friday, May 25 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Big $n$ regression\n",
    "\n",
    "Those who took my _203B: Introduction to Data Science_ last quarter had a (painful) experience of wrangling an Apache Spark cluster to do linear regression on a dataset with more than 100 million observations. Now we learnt various methods for solving linear regression and should realize that, with right choice of algorithm, it is a problem that can be handled by any moderate computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(1)\n",
    "\n",
    "Download the flight data from <http://stat-computing.org/dataexpo/2009/the-data.html>. For this exercise, we only need data from years 2003-2008. If you are using Mac or Linux, you can run the following Bash script, which downloads and unzips files for all years.\n",
    "```bash\n",
    "# Download flight data by year\n",
    "for i in {1987..2008}\n",
    "  do\n",
    "    echo \"$(date) $i Download\"\n",
    "    fnam=$i.csv.bz2\n",
    "    wget -O ./$fnam http://stat-computing.org/dataexpo/2009/$fnam\n",
    "    echo \"$(date) $i unzip\"\n",
    "    bzip2 -d ./$fnam\n",
    "  done\n",
    "\n",
    "# Download airline carrier data\n",
    "wget -O ./airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n",
    "\n",
    "# Download airports data\n",
    "wget -O ./airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n",
    "```\n",
    "Find out how many data points in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2003 = countlines(\"2003.csv\") = 6488541\n",
      "c2004 = countlines(\"2004.csv\") = 7129271\n",
      "c2005 = countlines(\"2005.csv\") = 7140597\n",
      "c2006 = countlines(\"2006.csv\") = 7141923\n",
      "c2007 = countlines(\"2007.csv\") = 7453216\n",
      "c2008 = countlines(\"2008.csv\") = 7009729\n",
      "total_lines = c2003 + c2004 + c2005 + c2006 + c2007 + c2008 = 42363277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42363277"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show c2003 = countlines(\"2003.csv\")\n",
    "@show c2004 = countlines(\"2004.csv\")\n",
    "@show c2005 = countlines(\"2005.csv\")\n",
    "@show c2006 = countlines(\"2006.csv\")\n",
    "@show c2007 = countlines(\"2007.csv\")\n",
    "@show c2008 = countlines(\"2008.csv\")\n",
    "@show total_lines = c2003 + c2004 + c2005 + c2006 + c2007 + c2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(2) \n",
    "\n",
    "We are interested in how the time gain of a flight, defined as `DepDelay - ArrDelay`, depends on the distance traveled (`Distance`), departure delay (`DepDelay`), and carrier (`UniqueCarrier`). \n",
    "\n",
    "We want to fit a linear regression `Gain ~ 1 + Distance + DepDelay + UniqueCarrier` using data from 2003-2008. Note `UniqueCarrier` is a factor with 23 levels: \"9E\", \"AA\", \"AQ\", \"AS\", \"B6\", \"CO\", \"DH\", \"DL\", \"EV\", \"F9\", \"FL\", \"HA\", \"HP\", \"MQ\", \"NW\", \"OH\", \"OO\", \"TZ\", \"UA\", \"US\", \"WN\", \"XE\", and \"YV\". We use the dummy coding with \"9E\" as base level.\n",
    "\n",
    "Will the design matrix (in double precision) fit into the memory of you computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Data from 2003-2008 contain 42363277 observations. The design matrix has dimension $42363277 \\times 26$, so in total $42363277 \\times 26$ of double-precision numbers. Every double-precision number is 8 bytes. We need a memory of $42363277 \\times 26 \\times 8 / 10^9 \\sim $8.81GB to fit the whole matrix. My poor MacBook with a 8GB memory can not handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(3)\n",
    "\n",
    "Review the [Summary of Linear Regression](http://hua-zhou.github.io/teaching/biostatm280-2018spring/slides/12-linreg/linreg.html) and devise a strategy to solve the linear regression.\n",
    "\n",
    "Report the estimated regression coefficients $\\widehat \\beta$, estimated variance $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1)$, and coefficient standard errors.\n",
    "\n",
    "Hint: It took my laptop less than 3 minutes to import data and fit linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       "  9.589    -4.98208  -5.70052  -1.83933\n",
       " -4.98208   4.94476   3.81663   2.82462\n",
       " -5.70052   3.81663   5.45442   2.46008\n",
       " -1.83933   2.82462   2.46008   4.98343"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SweepOperator\n",
    "\n",
    "srand(280)\n",
    "\n",
    "X = randn(5, 3) # predictor matrix\n",
    "y = randn(5)    # response vector\n",
    "\n",
    "# form the augmented Gram matrix\n",
    "G = [X y]' * [X y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant var2col\n",
      "WARNING: redefining constant col2var\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generate_xy (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from variable names to X columns\n",
    "# carrier \"9E\" is used as base level\n",
    "const var2col = Dict(\n",
    "        \"Intercept\" => 1,\n",
    "        \"Distance\" => 2,\n",
    "        \"DepDelay\" => 3,\n",
    "        \"AA\" => 4,\n",
    "        \"AQ\" => 5,\n",
    "        \"AS\" => 6,\n",
    "        \"B6\" => 7,\n",
    "        \"CO\" => 8,\n",
    "        \"DH\" => 9,\n",
    "        \"DL\" => 10,\n",
    "        \"EV\" => 11,\n",
    "        \"F9\" => 12,\n",
    "        \"FL\" => 13,\n",
    "        \"HA\" => 14,\n",
    "        \"HP\" => 15,\n",
    "        \"MQ\" => 16,\n",
    "        \"NW\" => 17,\n",
    "        \"OH\" => 18,\n",
    "        \"OO\" => 19,\n",
    "        \"TZ\" => 20,\n",
    "        \"UA\" => 21,\n",
    "        \"US\" => 22,\n",
    "        \"WN\" => 23,\n",
    "        \"XE\" => 24,\n",
    "        \"YV\" => 25,\n",
    "        \"Gain\" => 26)\n",
    "# mapping from column to variable names\n",
    "const col2var = map(reverse, var2col)\n",
    "\n",
    "# a custom function to generate [X y] from data table\n",
    "function generate_xy(tbl::NextTable)\n",
    "    # X matrix\n",
    "    XY = zeros(length(tbl), 26)\n",
    "    # intercept term\n",
    "    @views fill!(XY[:, 1], 1)\n",
    "    # Distance term\n",
    "    @views copy!(XY[:, 2], columns(tbl, :Distance))\n",
    "    # DepDelay term\n",
    "    @views copy!(XY[:, 3], columns(tbl, :DepDelay))\n",
    "    # Dummy coding for airline\n",
    "    @inbounds for i in 1:length(tbl)\n",
    "        tbl[i][:UniqueCarrier] == \"9E\" && continue # base level\n",
    "        XY[i, var2col[tbl[i][:UniqueCarrier]]] = 1\n",
    "    end\n",
    "    # last column is response: gain = depdelay - arrdelay\n",
    "    XY[:, 26] = select(tbl, \n",
    "        (:DepDelay, :ArrDelay) => p -> Float64(p.DepDelay - p.ArrDelay))\n",
    "    # return\n",
    "    XY\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48.905613 seconds (153.54 M allocations: 8.160 GiB, 25.52% gc time)\n",
      " 34.437851 seconds (31.53 M allocations: 4.276 GiB, 6.43% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Table with 7129270 rows, 4 columns:\n",
       "DepDelay  ArrDelay  UniqueCarrier  Distance\n",
       "───────────────────────────────────────────\n",
       "-7        -14       \"UA\"           599\n",
       "-9        -4        \"UA\"           599\n",
       "3         5         \"UA\"           599\n",
       "-3        -16       \"UA\"           599\n",
       "5         3         \"UA\"           599\n",
       "-2        -10       \"UA\"           599\n",
       "20        29        \"UA\"           599\n",
       "-3        -11       \"UA\"           599\n",
       "-7        -12       \"UA\"           599\n",
       "-4        -14       \"UA\"           599\n",
       "1         8         \"UA\"           599\n",
       "-2        -14       \"UA\"           599\n",
       "⋮\n",
       "0         -4        \"DL\"           821\n",
       "-2        -9        \"DL\"           590\n",
       "-2        -10       \"DL\"           532\n",
       "-1        6         \"DL\"           391\n",
       "2         -15       \"DL\"           599\n",
       "-6        -15       \"DL\"           215\n",
       "6         -1        \"DL\"           425\n",
       "6         -1        \"DL\"           1541\n",
       "6         3         \"DL\"           547\n",
       "-2        -3        \"DL\"           545\n",
       "-2        -13       \"DL\"           594"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuliaDB\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "@time yrtable2003 = loadtable(\n",
    "    \"2003.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "@time yrtable2004 = loadtable(\n",
    "    \"2004.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26×26 Array{Float64,2}:\n",
       "      6.37569e6  4.5497e9         3.32999e7   …  0.0        1.03685e7 \n",
       "      4.5497e9   5.28186e12       2.37964e10     0.0        1.01318e10\n",
       "      3.32999e7  2.37964e10       4.51178e9      0.0        4.30844e7 \n",
       " 738567.0        7.7143e8         3.42572e6      0.0        1.05689e6 \n",
       "      0.0        0.0              0.0            0.0        0.0       \n",
       " 157602.0        1.31742e8   895388.0         …  0.0   394580.0       \n",
       "  66732.0        8.49327e7   377211.0            0.0   311411.0       \n",
       " 299843.0        3.24764e8        1.1675e6       0.0    33212.0       \n",
       " 280010.0        1.00559e8        2.63428e6      0.0   804746.0       \n",
       " 652602.0        5.2721e8         2.95877e6      0.0   442798.0       \n",
       " 266760.0        1.08515e8        2.37215e6   …  0.0   595560.0       \n",
       "      0.0        0.0              0.0            0.0        0.0       \n",
       " 142817.0        8.51543e7   983078.0            0.0  -211739.0       \n",
       "   7792.0        4.77305e6     8050.0            0.0     9164.0       \n",
       " 187266.0        1.86801e8        1.08765e6      0.0   514332.0       \n",
       " 414902.0        1.38511e8        2.07969e6   …  0.0   320786.0       \n",
       " 492274.0        3.64361e8        1.06672e6      0.0   100264.0       \n",
       "      0.0        0.0              0.0            0.0        0.0       \n",
       " 389762.0        1.28088e8        2.11875e6      0.0        1.22713e6 \n",
       "  68265.0        7.81216e7   323703.0            0.0   -14864.0       \n",
       " 537178.0        5.58834e8        2.50021e6   …  0.0   827482.0       \n",
       " 404988.0        2.70963e8        1.67781e6      0.0   -32494.0       \n",
       " 947231.0        5.28468e8        6.30219e6      0.0        4.34724e6 \n",
       " 321098.0        1.56471e8        1.321e6        0.0  -358014.0       \n",
       "      0.0        0.0              0.0            0.0        0.0       \n",
       "      1.03685e7  1.01318e10       4.30844e7   …  0.0        1.51637e9 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SweepOperator\n",
    "yrtable2003 = dropna(yrtable2003)\n",
    "yrtable2004 = dropna(yrtable2004)\n",
    "g3 = generate_xy(yrtable2003)\n",
    "g4 = generate_xy(yrtable2004)\n",
    "g = g3'*g3 \n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26×26 Array{Float64,2}:\n",
       "    NaN          NaN           …  NaN              NaN    NaN    NaN\n",
       "      4.5497e9   NaN              NaN              NaN    NaN    NaN\n",
       "      3.32999e7    2.37964e10     NaN              NaN    NaN    NaN\n",
       " 738567.0          7.7143e8       NaN              NaN    NaN    NaN\n",
       "      0.0          0.0            NaN              NaN    NaN    NaN\n",
       " 157602.0          1.31742e8   …  NaN              NaN    NaN    NaN\n",
       "  66732.0          8.49327e7      NaN              NaN    NaN    NaN\n",
       " 299843.0          3.24764e8      NaN              NaN    NaN    NaN\n",
       " 280010.0          1.00559e8      NaN              NaN    NaN    NaN\n",
       " 652602.0          5.2721e8       NaN              NaN    NaN    NaN\n",
       " 266760.0          1.08515e8   …  NaN              NaN    NaN    NaN\n",
       "      0.0          0.0            NaN              NaN    NaN    NaN\n",
       " 142817.0          8.51543e7      NaN              NaN    NaN    NaN\n",
       "   7792.0          4.77305e6      NaN              NaN    NaN    NaN\n",
       " 187266.0          1.86801e8      NaN              NaN    NaN    NaN\n",
       " 414902.0          1.38511e8   …  NaN              NaN    NaN    NaN\n",
       " 492274.0          3.64361e8      NaN              NaN    NaN    NaN\n",
       "      0.0          0.0            NaN              NaN    NaN    NaN\n",
       " 389762.0          1.28088e8      NaN              NaN    NaN    NaN\n",
       "  68265.0          7.81216e7      NaN              NaN    NaN    NaN\n",
       " 537178.0          5.58834e8   …  NaN              NaN    NaN    NaN\n",
       " 404988.0          2.70963e8      NaN              NaN    NaN    NaN\n",
       " 947231.0          5.28468e8      NaN              NaN    NaN    NaN\n",
       " 321098.0          1.56471e8        0.0            NaN    NaN    NaN\n",
       "      0.0          0.0              0.0              0.0  NaN    NaN\n",
       "      1.03685e7    1.01318e10  …    4.34724e6  -358014.0    0.0  NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep!(g, 1:25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247.965442 seconds (483.02 M allocations: 44.730 GiB, 18.21% gc time)\n"
     ]
    }
   ],
   "source": [
    "# maybe a function\n",
    "using JuliaDB\n",
    "# pre-allocate a matrix gram_augmented  \n",
    "n = 0\n",
    "p = 26\n",
    "\n",
    "gram_augmented = zeros(p, p)\n",
    "gram_augmented_sum = zeros(p, p)\n",
    "@time for year = 2003:2008\n",
    "    filename = join([string(year),\".csv\"]) \n",
    "    yrtable = loadtable(\n",
    "        filename, \n",
    "        datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "    yrtable = dropna(yrtable)\n",
    "    xy_matrix = generate_xy(yrtable)\n",
    "    n += size(xy_matrix, 1)\n",
    "    gram_augmented = At_mul_B(xy_matrix, xy_matrix)\n",
    "    gram_augmented_sum += gram_augmented\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26×26 Array{Float64,2}:\n",
       "     -2.00254e-6   4.21792e-11  …   1.98347e-6   -1.14033   \n",
       "      3.00028e10  -9.38376e-14     -5.01596e-12  -0.00164935\n",
       "      3.69821e8    2.76146e11       1.16836e-10   0.0118811 \n",
       "      3.91626e6    4.19149e9       -1.97927e-6    1.8723    \n",
       "  88336.0          3.75197e7       -1.98134e-6    0.5789    \n",
       " 937547.0          8.30881e8    …  -1.98009e-6    0.938452  \n",
       " 799117.0          9.55599e8       -1.97878e-6    1.42247   \n",
       "      1.8094e6     2.03781e9       -1.97887e-6    2.57627   \n",
       " 669687.0          2.51484e8       -1.98271e-6   -1.16808   \n",
       "      3.37715e6    2.9514e9        -1.97993e-6    2.19625   \n",
       "      1.64509e6    7.42218e8    …  -1.98278e-6   -1.03932   \n",
       " 334842.0          2.97744e8       -1.97972e-6    2.15207   \n",
       "      1.24932e6    8.33171e8       -1.98132e-6    1.35247   \n",
       " 272859.0          1.61374e8       -1.98044e-6    1.87248   \n",
       " 574927.0          5.84803e8       -1.97917e-6    0.350758  \n",
       "      2.90986e6    1.13812e9    …  -1.98262e-6    1.46395   \n",
       "      2.64244e6    2.01399e9       -1.98034e-6    3.62506   \n",
       "      1.41412e6    6.64003e8       -1.9822e-6     0.00722279\n",
       "      3.02053e6    1.17539e9       -1.98236e-6    0.40365   \n",
       " 206007.0          2.37097e8       -1.97834e-6    3.5774    \n",
       "      2.96794e6    3.19798e9    …  -1.97923e-6    1.14816   \n",
       "      2.65381e6    2.03684e9       -1.98051e-6    0.883753  \n",
       "      6.38387e6    3.87575e9       -1.98156e-6   -2.74855   \n",
       "      2.29046e6    1.23256e9       -1.98175e-6    2.56721   \n",
       " 822290.0          3.27927e8       -3.19909e-6    0.202211  \n",
       "      6.0236e7     5.75882e10   …   1.18638e6     8.48076e9 "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SweepOperator\n",
    "sweep_result = sweep!(gram_augmented_sum, 1:(p - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) continued ** Recall that sweeping on the Gram matrix yields \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\begin{pmatrix}\n",
    "- (\\mathbf{X}^T \\mathbf{X})^{-1} & (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} \\\\\n",
    "\\mathbf{y}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} & \\mathbf{y}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "- \\sigma^{-2} \\text{Cov}(\\widehat \\beta) & \\widehat \\beta \\\\\n",
    "\\widehat \\beta^T & \\|\\mathbf{y} - \\hat y\\|_2^2\n",
    "\\end{pmatrix}.\n",
    "\\end{eqnarray*}\n",
    "$$ \n",
    "\n",
    "The estimated regression coefficients $\\widehat \\beta$, estimated variance $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1)$, and coefficient standard errors can be easily gained from the sweeped matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\widehat \\beta =$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Array{Float64,1}:\n",
       " -1.14033   \n",
       " -0.00164935\n",
       "  0.0118811 \n",
       "  1.8723    \n",
       "  0.5789    \n",
       "  0.938452  \n",
       "  1.42247   \n",
       "  2.57627   \n",
       " -1.16808   \n",
       "  2.19625   \n",
       " -1.03932   \n",
       "  2.15207   \n",
       "  1.35247   \n",
       "  1.87248   \n",
       "  0.350758  \n",
       "  1.46395   \n",
       "  3.62506   \n",
       "  0.00722279\n",
       "  0.40365   \n",
       "  3.5774    \n",
       "  1.14816   \n",
       "  0.883753  \n",
       " -2.74855   \n",
       "  2.56721   \n",
       "  0.202211  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweepresult[1:p - 1, p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1) = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.40310742569457"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma2 = sweepresult[p, p] / (n - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Array{Float64,1}:\n",
       " 0.0202318 \n",
       " 4.37958e-6\n",
       " 6.88878e-5\n",
       " 0.0215571 \n",
       " 0.0521477 \n",
       " 0.0250361 \n",
       " 0.0259154 \n",
       " 0.0229534 \n",
       " 0.0266552 \n",
       " 0.021659  \n",
       " 0.023012  \n",
       " 0.0319265 \n",
       " 0.023868  \n",
       " 0.0339852 \n",
       " 0.0276918 \n",
       " 0.0218055 \n",
       " 0.0220088 \n",
       " 0.023446  \n",
       " 0.0217463 \n",
       " 0.0375073 \n",
       " 0.0219444 \n",
       " 0.0220022 \n",
       " 0.0209201 \n",
       " 0.0222383 \n",
       " 0.0255715 "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt.(diag(sweepresult[1:p-1, 1:p-1] * (-sigma2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(4)\n",
    "\n",
    "Go to your resume/cv and claim you have experience performing analytics on data with hundred millions of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Google PageRank\n",
    "\n",
    "We are going to try different numerical methods learnt in class on the [Google PageRank problem](https://en.wikipedia.org/wiki/PageRank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(1)\n",
    "\n",
    "Let $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$ be the connectivity matrix of $n$ web pages with entries\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\ta_{ij}= \\begin{cases}\n",
    "\t1 & \\text{if page $i$ links to page $j$} \\\\\n",
    "\t0 & \\text{otherwise}\n",
    "\t\\end{cases}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$r_i = \\sum_j a_{ij}$ is the out-degree of page $i$. That is $r_i$ is the number of links on page $i$. Imagine a random surfer exploring the space of $n$ pages according to the following rules.  \n",
    "\n",
    "- From a page $i$ with $r_i>0$\n",
    "    * with probability $p$, (s)he randomly chooses a link on page $i$ (uniformly) and follows that link to the next page  \n",
    "    * with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "- From a page $i$ with $r_i=0$ (a dangling page), (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "    \n",
    "The process defines a Markov chain on the space of $n$ pages. Write the transition matrix $\\mathbf{P}$ of the Markov chain as a sparse matrix plus rank 1 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q2(2)\n",
    "\n",
    "According to standard Markov chain theory, the (random) position of the surfer converges to the stationary distribution $\\mathbf{x} = (x_1,\\ldots,x_n)^T$ of the Markov chain. $x_i$ has the natural interpretation of the proportion of times the surfer visits page $i$ in the long run. Therefore $\\mathbf{x}$ serves as page ranks: a higher $x_i$ means page $i$ is more visited. It is well-known that $\\mathbf{x}$ is the left eigenvector corresponding to the top eigenvalue 1 of the transition matrix $\\mathbf{P}$. That is $\\mathbf{P}^T \\mathbf{x} = \\mathbf{x}$. Therefore $\\mathbf{x}$ can be solved as an eigen-problem. Show that it can also be cast as solving a linear system. Since the row sums of $\\mathbf{P}$ are 1, $\\mathbf{P}$ is rank deficient. We can replace the first equation by the $\\sum_{i=1}^n x_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\mathbf{P}^T -\\mathbf{I} )\\mathbf{x} = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(3)\n",
    "\n",
    "Download the [`ucla.zip`](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw3/ucla.zip) package from course webpage. Unzip the package, which contains two files `U.txt` and `A.txt`. `U.txt` lists the 500 URL names. `A.txt` is the $500 \\times 500$ connectivity matrix. Read data into Julia. Compute summary statistics:\n",
    "* number of pages\n",
    "* number of edges\n",
    "* number of dangling nodes (pages with no out links)\n",
    "* which page has max in-degree?\n",
    "* which page has max out-degree?\n",
    "* visualize the sparsity pattern of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(4)\n",
    "\n",
    "Set the _teleportation_ parameter at $p = 0.85$. Try the following methods to solve for $\\mathbf{x}$ using the `ucla.zip` data.\n",
    "\n",
    "0. A dense linear system solver such as LU decomposition.  \n",
    "0. A simple iterative linear system solver such as Jacobi or Gauss-Seidel.   \n",
    "0. A dense eigen-solver.  \n",
    "0. A simple iterative eigen-solver such as the power method.  \n",
    "\n",
    "For iterative methods, you can use the [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package. Make sure to utilize the special structure of $\\mathbf{P}$ (sparse + rank 1) to speed up the matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(5)\n",
    "\n",
    "List the top 20 ranked URLs you found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(6)\n",
    "\n",
    "As of Monday May 11 2018, there are at least 1.83 billion indexed webpages on internet according to <http://www.worldwidewebsize.com/>. Explain whether each of these methods works for the PageRank problem at this scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
